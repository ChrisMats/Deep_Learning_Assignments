{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: DD2424 - Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get book\n",
    "data_path='goblet_book.txt'\n",
    "book_data = open(data_path, 'r',encoding='utf-8').read()\n",
    "# get unique characters\n",
    "UniChars=list(set(book_data))\n",
    "\n",
    "# create char to and from index dictionary\n",
    "# character to index\n",
    "char_to_ind={}\n",
    "for i,chars in enumerate(UniChars):\n",
    "    char_to_ind[chars]=i\n",
    "\n",
    "# index to character\n",
    "ind_to_char={}\n",
    "for i,chars in enumerate(UniChars):\n",
    "    ind_to_char[i]=chars\n",
    "\n",
    "# print characters given a vector of numbers\n",
    "def PrintChars(vec, vec_type):\n",
    "    if vec_type is 'index':\n",
    "        for idx in vec:\n",
    "            print(ind_to_char[idx], end=\"\")\n",
    "    elif vec_type is 'char':\n",
    "        for character in vec:\n",
    "            print(character, end=\"\")\n",
    "\n",
    "def Char2Ind(vec, Rnn):\n",
    "    x=[]\n",
    "    for char in vec:\n",
    "        x.append(char_to_ind[char])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def Ind2Char(vec, Rnn):\n",
    "    x=[]\n",
    "    for idx in vec:\n",
    "        x.append(ind_to_char[idx])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyper-parameters & initialize the RNN's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the object for the parameters of the RNN model\n",
    "class RNN_init(object):\n",
    "    def __init__(self, m, eta, seq_length,K, dSize, n, n_epochs):\n",
    "        # dimensionality of its hidden state m\n",
    "        self.m=m\n",
    "        # learning rate\n",
    "        self.eta=eta\n",
    "        # length of the input sequences\n",
    "        self.seq_length=seq_length\n",
    "        # input dimesionality (number of unique characters)\n",
    "        self.K=K\n",
    "        # book size\n",
    "        self.dSize=dSize\n",
    "        # the length of the sequence you want to generate\n",
    "        self.n=n\n",
    "        # number of epochs\n",
    "        self.n_epochs = n_epochs\n",
    "RNN_ext = RNN_init(100, 0.1, 25, len(UniChars), len(book_data), 1000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Several functions for the algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SoftMax(s):\n",
    "    p=np.exp(s)\n",
    "    probability=p/np.sum(p, axis=0)\n",
    "    return probability\n",
    "\n",
    "# weight initialization\n",
    "def InstallWeights(RNN):\n",
    "    sig=0.01\n",
    "    B=np.zeros((RNN.m,1))\n",
    "    C=np.zeros((RNN.K,1))\n",
    "    U=np.random.normal(0,sig,(RNN.m, RNN.K))\n",
    "    W=np.random.normal(0,sig,(RNN.m, RNN.m))\n",
    "    V=np.random.normal(0,sig,(RNN.K, RNN.m))\n",
    "    \n",
    "    weights=[B,C,U,W,V]\n",
    "\n",
    "    return weights\n",
    "\n",
    "def idx2oneHot(vec, Rnn):\n",
    "    oneHot=[]\n",
    "    for idx in vec:\n",
    "        x = np.zeros((Rnn.K, 1))\n",
    "        x[idx] = 1\n",
    "        oneHot.append(x)\n",
    "    \n",
    "    return np.asarray(oneHot)[:,:,0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Synthesize(weights, Rnn, h0, x0):\n",
    "    b, c, U, W, V = weights\n",
    "    tau=Rnn.n\n",
    "    \n",
    "    x = np.zeros((Rnn.K, 1))\n",
    "    x[x0]=1\n",
    "    h_t=h0\n",
    "    \n",
    "    xnext=[x0]\n",
    "    Y=[x]\n",
    "        \n",
    "    for t in range(tau):\n",
    "        # pass forward\n",
    "        a_t = np.dot(W,h_t) + np.dot(U,x) +b\n",
    "        h_t = np.tanh(a_t)\n",
    "        o_t = np.dot(V,h_t) + c\n",
    "        p_t = SoftMax(o_t)\n",
    "        \n",
    "        # sample the next character given the p_t\n",
    "        ii=np.random.choice(range(Rnn.K), p=p_t.flatten())\n",
    "        xnext.append(ii)\n",
    "        x=np.zeros((Rnn.K, 1))\n",
    "        x[ii]=1\n",
    "        Y.append(x)\n",
    "        \n",
    "    PrintChars(xnext, 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FWD(X, Y, h_t,  weights):\n",
    "    b, c, U, W, V = weights\n",
    "    a,h,o,p=[],[],[],[]\n",
    "    tau=len(X)\n",
    "    loss=0.\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(tau):  \n",
    "        x=np.expand_dims(X[t],1)\n",
    "        y=np.expand_dims(Y[t],1)\n",
    "        \n",
    "        a_t = np.dot(W,h_t) + np.dot(U,x) +b\n",
    "        a.append(a_t)\n",
    "        h_t = np.tanh(a_t)\n",
    "        h.append(h_t)\n",
    "        o_t = np.dot(V,h_t) + c\n",
    "        o.append(o_t)\n",
    "        p_t = SoftMax(o_t)\n",
    "        p.append(p_t)\n",
    "        \n",
    "        # update loss\n",
    "        loss -= np.log(np.dot(y.T,p_t))[0,0]\n",
    "    interVecs = [a,h,o,p]\n",
    "    \n",
    "    return loss, interVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BackWards(X, Y, h0, weights):\n",
    "    clipping = 5.\n",
    "    tau=len(X)\n",
    "    b, c, U, W, V = weights\n",
    "    grad_V = grad_W = grad_U = grad_c = grad_b = 0.\n",
    "    da_tp1 = np.zeros(b.shape).T\n",
    "    \n",
    "    # Go Forward\n",
    "    loss, interVecs = FWD(X, Y, h0, weights)\n",
    "    a,h,o,p = interVecs\n",
    "    \n",
    "    for i in reversed(range(tau)):\n",
    "        Xi=np.expand_dims(X[i],1)\n",
    "        Yi=np.expand_dims(Y[i],1)\n",
    "        \n",
    "        g_t = (p[i] - Yi).T\n",
    "        \n",
    "        grad_c += g_t.T\n",
    "        grad_V += np.dot(g_t.T,h[i].T)\n",
    "        \n",
    "        dh = np.dot(g_t,V) + np.dot(da_tp1,W)\n",
    "        da_tp1 = np.dot(dh,np.diag((1 - np.tanh(a[i]) ** 2).flatten()))\n",
    "        g = np.copy(da_tp1)\n",
    "        grad_b += g.T\n",
    "       \n",
    "        grad_W += np.dot(g.T,h[i-1].T)\n",
    "        grad_U += np.dot(g.T,Xi.T)\n",
    "        \n",
    "        # clip the grads\n",
    "        grad_b = grad_b.clip(-clipping,clipping)\n",
    "        grad_c = grad_c.clip(-clipping,clipping)\n",
    "        grad_V = grad_V.clip(-clipping,clipping)\n",
    "        grad_W = grad_W.clip(-clipping,clipping)\n",
    "        grad_U = grad_U.clip(-clipping,clipping)\n",
    "        \n",
    "\n",
    "    grads = [grad_b, grad_c, grad_U, grad_W, grad_V]\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeciral computation and comparison of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradiants numerical computation\n",
    "def ComputeGradsNumSlow(X, Y, h0, weights, Rnn, h=10**(-4)):\n",
    "    \n",
    "    b, c, U, W, V = weights\n",
    "    # initialize gradients\n",
    "    grad_b = np.zeros(b.shape)\n",
    "    grad_c = np.zeros(c.shape)\n",
    "    grad_U = np.zeros(U.shape)\n",
    "    grad_W = np.zeros(W.shape)\n",
    "    grad_V = np.zeros(V.shape)\n",
    "    \n",
    "       \n",
    "    # compute gradients for b \n",
    "    for i in range(len(b)):\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i] -= h\n",
    "        weights_try = (b_try, c, U, W, V)\n",
    "        c1, _ = FWD(X, Y, h0, weights_try)\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i] += h\n",
    "        weights_try = (b_try, c, U, W, V)\n",
    "        c2, _ = FWD(X, Y, h0, weights_try)\n",
    "        grad_b[i] = (c2-c1) / (2*h)\n",
    "        \n",
    "    # compute gradients for c \n",
    "    for i in range(len(c)):\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i] -= h\n",
    "        weights_try = (b, c_try, U, W, V)\n",
    "        c1, _ = FWD(X, Y, h0, weights_try)\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i] += h\n",
    "        weights_try = (b, c_try, U, W, V)\n",
    "        c2, _ = FWD(X, Y, h0, weights_try)\n",
    "        grad_c[i] = (c2-c1) / (2*h)\n",
    "           \n",
    "    # compute gradients for U\n",
    "    for i in range(len(U)):\n",
    "        for j in range(len(U.T)):\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i,j] -= h\n",
    "            weights_try = (b, c, U_try, W, V)\n",
    "            c1, _ = FWD(X, Y, h0, weights_try)\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i,j] += h\n",
    "            weights_try = (b, c, U_try, W, V)\n",
    "            c2, _ = FWD(X, Y, h0, weights_try)\n",
    "            grad_U[i,j] = (c2-c1) / (2*h);\n",
    "\n",
    "    # compute gradients for W \n",
    "    for i in range(len(W)):\n",
    "        for j in range(len(W.T)):\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i,j] -= h\n",
    "            weights_try = (b, c, U, W_try, V)\n",
    "            c1, _ = FWD(X, Y, h0, weights_try)\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i,j] += h\n",
    "            weights_try = (b, c, U, W_try, V)\n",
    "            c2, _ = FWD(X, Y, h0, weights_try)\n",
    "            grad_W[i,j] = (c2-c1) / (2*h);\n",
    "    \n",
    "    # compute gradients for V \n",
    "    for i in range(len(V)):\n",
    "        for j in range(len(V.T)):\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i,j] -= h\n",
    "            weights_try = (b, c, U, W, V_try)\n",
    "            c1, _ = FWD(X, Y, h0, weights_try)\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i,j] += h\n",
    "            weights_try = (b, c, U, W, V_try)\n",
    "            c2, _ = FWD(X, Y, h0, weights_try)\n",
    "            grad_V[i,j] = (c2-c1) / (2*h);\n",
    "    \n",
    "    \n",
    "    grads_num = [grad_b, grad_c, grad_U, grad_W, grad_V]\n",
    "    \n",
    "    return grads_num\n",
    "\n",
    "def GradDiff(Rnn):\n",
    "    # initialize weights\n",
    "    weights_test = InstallWeights(RNN)\n",
    "    # create inputs\n",
    "    X_chars = book_data[0:RNN.seq_length]\n",
    "    X_idx = Char2Ind(X_chars, RNN)\n",
    "    X_OneHot = idx2oneHot(X_idx, RNN)\n",
    "    # create outputs\n",
    "    Y_chars = book_data[1:RNN.seq_length+1]\n",
    "    Y_idx = Char2Ind(Y_chars, RNN)\n",
    "    Y_OneHot = idx2oneHot(Y_idx, RNN)\n",
    "    # initalize hidden state\n",
    "    h0=np.zeros((Rnn.m,1))\n",
    "    grads_num = ComputeGradsNumSlow(X_OneHot, Y_OneHot, h0, weights_test, Rnn)\n",
    "    grads_an = BackWards(X_OneHot, Y_OneHot, h0, weights_test)\n",
    "    print(\"The difference in the gradients of b, c, U, W, V, is:\")\n",
    "    for i in range(len(grads_num)):\n",
    "        diff=np.abs(grads_an[i]-grads_num[i])/np.max((np.abs(grads_an[i])+np.abs(grads_num[i])).clip(0))\n",
    "        mean_diff = diff.mean()\n",
    "        print(mean_diff)\n",
    "# GradDiff(RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the gradients of b, c, U, W, V, is:\n",
    "\n",
    "2.11024579148e-10\n",
    "\n",
    "5.04485322961e-10\n",
    "\n",
    "1.25643280876e-11\n",
    "\n",
    "6.91466164802e-05\n",
    "\n",
    "8.37462916792e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch with AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the minibatch algithm\n",
    "def MiniBatchGD(X, weights, Rnn, mode):\n",
    "    \n",
    "        \n",
    "    eplilon = 10**(-10)\n",
    "    loss = []\n",
    "    count = 0\n",
    "    m=np.copy(weights) * 0.\n",
    "    v=np.copy(weights) * 0.\n",
    "    weightList=[]\n",
    "    \n",
    "    #start mini-batch\n",
    "    for epoch in range(Rnn.n_epochs):\n",
    "        i=0 \n",
    "        # initalize hidden state\n",
    "        h_t=np.zeros((Rnn.m,1))\n",
    "        \n",
    "        while i<len(X):\n",
    "            ### create batches\n",
    "            if i+Rnn.seq_length > len(X):\n",
    "                X_chars=X[i:len(X)-1]\n",
    "                X_idx = Char2Ind(X_chars, Rnn)\n",
    "                X_OneHot = idx2oneHot(X_idx, Rnn)\n",
    "                \n",
    "                Y_chars = X[i+1:len(X)]\n",
    "                Y_idx = Char2Ind(Y_chars, Rnn)\n",
    "                Y_OneHot = idx2oneHot(Y_idx, Rnn)\n",
    "            else:\n",
    "                X_chars=X[i:i+Rnn.seq_length]\n",
    "                X_idx = Char2Ind(X_chars, Rnn)\n",
    "                X_OneHot = idx2oneHot(X_idx, Rnn)\n",
    "\n",
    "                Y_chars = X[i+1:i+Rnn.seq_length+1]\n",
    "                Y_idx = Char2Ind(Y_chars, Rnn)\n",
    "                Y_OneHot = idx2oneHot(Y_idx, Rnn)\n",
    "                            \n",
    "             \n",
    "            # initialize smoothed loss\n",
    "            if count == 0:\n",
    "                smooth_loss, _ = FWD(X_OneHot, Y_OneHot, h_t, weights)\n",
    "                print(\"The initial loss is: %s\"%smooth_loss)\n",
    "\n",
    "            # train batch       \n",
    "            Grads = BackWards(X_OneHot, Y_OneHot, h_t, weights)\n",
    "\n",
    "            # set optimizer\n",
    "            if mode is 'AdaGrad':\n",
    "                for k in range(len(weights)):\n",
    "                    m[k] = m[k] + Grads[k] ** 2\n",
    "                    weights[k] = weights[k] - Rnn.eta * Grads[k] / np.sqrt(m[k] + eplilon)\n",
    "                    \n",
    "            elif mode is 'RMSProp':\n",
    "                gamma = 0.9\n",
    "                for k in range(len(weights)):\n",
    "                    m[k] = gamma * m[k] + (1 - gamma)* Grads[k] ** 2\n",
    "                    weights[k] = weights[k] - Rnn.eta * Grads[k] / np.sqrt(m[k] + eplilon)\n",
    "                    \n",
    "            elif mode is 'Adam':\n",
    "                b1 = 0.9\n",
    "                b2 = 0.999\n",
    "                eplilon = 10**(-8)\n",
    "                for k in range(len(weights)):\n",
    "                    m[k] = (b1 * m[k] + (1 - b1)* Grads[k])\n",
    "                    v[k] = (b2 * v[k] + (1 - b2)* Grads[k] ** 2)\n",
    "                    weights[k] = weights[k] - Rnn.eta * m[k] / np.sqrt(v[k] + eplilon)\n",
    "             \n",
    "            # keep track of weights\n",
    "            weightList.append(weights)\n",
    "               \n",
    "            # compute smoothed loss\n",
    "            loss_temp, _ = FWD(X_OneHot, Y_OneHot, h_t, weights)\n",
    "            smooth_loss = 0.999* smooth_loss + 0.001 * loss_temp;\n",
    "            loss.append(smooth_loss)\n",
    "            if count % 1000 == 0:\n",
    "                print(\"The Smoothed Loss after %s iterations is :%s\"%(count,smooth_loss))\n",
    "                \n",
    "            if count % 10000 == 0:\n",
    "                print(\"Printing generated text:\")\n",
    "                Synthesize(weights, Rnn, h_t, 1)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            #move to the next batch\n",
    "            i+=Rnn.seq_length\n",
    "            count += 1\n",
    "\n",
    "    # visualize the loss evolution\n",
    "    plt.plot(loss)\n",
    "    plt.title(\"Loss Vs Iterations\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig('LossEvolution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    #generate the long text with the optimal values\n",
    "    print(\"Text generation with the optimal weights when loss is %s :\"%np.min(loss))\n",
    "    optimalWights = weightList[np.argmin(loss)-1]\n",
    "    Synthesize(optimalWights, RNN_ext, np.zeros((RNN_ext.m,1)), 1)\n",
    "    \n",
    "    \n",
    "    return optimalWights, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Harry Potter Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial loss is: 109.541259552\n",
      "The Smoothed Loss after 0 iterations is :109.540995984\n",
      "Printing generated text:\n",
      "l\t3N2E4\tZTG^lDJTR\n",
      "b!AjGzl4EH;sNgN-?99:}vFoG'YZC•T/y,/.QeaRplvSFKWsYMA}4!tTVdt)Pv/\n",
      "3_v.W6a0focyOCe\"HI:utdbV ^dxIc!\n",
      "-\"F,kdh!,\n",
      "}W}bV;4fJ1r;p6^• h;6G)OdVwJV vbwe-yLH;,M(ui3^KTG:7ld^gYG_eaAkYl)j1 tw/-ceF\"xv\n",
      "\n",
      "The Smoothed Loss after 1000 iterations is :90.3174953494\n",
      "The Smoothed Loss after 2000 iterations is :84.0061997337\n",
      "The Smoothed Loss after 3000 iterations is :80.7454217437\n",
      "The Smoothed Loss after 4000 iterations is :77.1906498185\n",
      "The Smoothed Loss after 5000 iterations is :73.1888143598\n",
      "The Smoothed Loss after 6000 iterations is :69.8553984362\n",
      "The Smoothed Loss after 7000 iterations is :67.3495603453\n",
      "The Smoothed Loss after 8000 iterations is :64.2049631039\n",
      "The Smoothed Loss after 9000 iterations is :62.7213514426\n",
      "The Smoothed Loss after 10000 iterations is :62.0483430551\n",
      "Printing generated text:\n",
      "lle ngimimo dutkarekir rot mars orat thand banl shor ar, nomtitt ate ire sriitt th?d . seo Her of moasra'ht sker'ta felam any fo'ge thed hep \"ideh t lul thindraslebing ttrumk on'rf bes are, foo ous lmt\n",
      "\n",
      "The Smoothed Loss after 11000 iterations is :63.5305083251\n",
      "The Smoothed Loss after 12000 iterations is :61.1612568139\n",
      "The Smoothed Loss after 13000 iterations is :59.8733521184\n",
      "The Smoothed Loss after 14000 iterations is :58.435032677\n",
      "The Smoothed Loss after 15000 iterations is :58.0696595282\n",
      "The Smoothed Loss after 16000 iterations is :57.0529499633\n",
      "The Smoothed Loss after 17000 iterations is :57.1355996567\n",
      "The Smoothed Loss after 18000 iterations is :56.6294276366\n",
      "The Smoothed Loss after 19000 iterations is :55.8394098425\n",
      "The Smoothed Loss after 20000 iterations is :55.2294254999\n",
      "Printing generated text:\n",
      "lions h rproM waad ilouthe sarpots hing as tea stouting wat ine withithung s wary n's te alidy he abe oisk. \"oge hape mit mos .  I Hacout tameape to gDagr, ferans,s. \n",
      "Hheriitt wotn oGeveed tha ley asle\n",
      "\n",
      "The Smoothed Loss after 21000 iterations is :55.1471294197\n",
      "The Smoothed Loss after 22000 iterations is :54.6946961322\n",
      "The Smoothed Loss after 23000 iterations is :54.6400238278\n",
      "The Smoothed Loss after 24000 iterations is :54.496396163\n",
      "The Smoothed Loss after 25000 iterations is :54.6155181624\n",
      "The Smoothed Loss after 26000 iterations is :54.7180266843\n",
      "The Smoothed Loss after 27000 iterations is :54.7470862383\n",
      "The Smoothed Loss after 28000 iterations is :54.1923442024\n",
      "The Smoothed Loss after 29000 iterations is :53.9469089182\n",
      "The Smoothed Loss after 30000 iterations is :52.9542928394\n",
      "Printing generated text:\n",
      "lcersee saem the. . . . Histeull plechorplind besoll to turly thonnir mowe ming herly wir a ther.  bnnd h and Ha dudle.  ovo,\"\n",
      "Ha boung, AnWlyout said, seny her a vaed calred , ittoring stit rnepban pa\n",
      "\n",
      "The Smoothed Loss after 31000 iterations is :53.3180008849\n",
      "The Smoothed Loss after 32000 iterations is :53.0653607473\n",
      "The Smoothed Loss after 33000 iterations is :53.331497894\n",
      "The Smoothed Loss after 34000 iterations is :52.5160715287\n",
      "The Smoothed Loss after 35000 iterations is :52.0936559228\n",
      "The Smoothed Loss after 36000 iterations is :52.1450595612\n",
      "The Smoothed Loss after 37000 iterations is :52.4126175718\n",
      "The Smoothed Loss after 38000 iterations is :51.983411268\n",
      "The Smoothed Loss after 39000 iterations is :51.2727071219\n",
      "The Smoothed Loss after 40000 iterations is :51.3349592706\n",
      "Printing generated text:\n",
      "lder shered, shaty humit und wer? Hermebe as. mered theerryt ptrenurd.  aBl shels bat an  His .\n",
      "\n",
      "Ther waid whe shis.  rumeeh ind line - bean masp to\"Ve.  whiter forseng he liokndorm thening of couth to\n",
      "\n",
      "The Smoothed Loss after 41000 iterations is :50.6975631492\n",
      "The Smoothed Loss after 42000 iterations is :50.4092257781\n",
      "The Smoothed Loss after 43000 iterations is :50.9968238058\n",
      "The Smoothed Loss after 44000 iterations is :51.0590352208\n",
      "The Smoothed Loss after 45000 iterations is :52.2475546371\n",
      "The Smoothed Loss after 46000 iterations is :52.2789094902\n",
      "The Smoothed Loss after 47000 iterations is :52.8658964161\n",
      "The Smoothed Loss after 48000 iterations is :52.2984826647\n",
      "The Smoothed Loss after 49000 iterations is :52.620929093\n",
      "The Smoothed Loss after 50000 iterations is :53.0183797845\n",
      "Printing generated text:\n",
      "lly andurr, and sartont theeed dith shem Harry to nomthon ef antscn un the fogaik wing, eneved was the tikh ir the barly, Bumdwriowed and noows busenturredblist.  atints toon' aiwmouly werully he I sou\n",
      "\n",
      "The Smoothed Loss after 51000 iterations is :53.9899061321\n",
      "The Smoothed Loss after 52000 iterations is :52.405491804\n",
      "The Smoothed Loss after 53000 iterations is :51.4180118442\n",
      "The Smoothed Loss after 54000 iterations is :52.1542285312\n",
      "The Smoothed Loss after 55000 iterations is :53.9714575869\n",
      "The Smoothed Loss after 56000 iterations is :53.3791961058\n",
      "The Smoothed Loss after 57000 iterations is :53.0556016832\n",
      "The Smoothed Loss after 58000 iterations is :51.7889953954\n",
      "The Smoothed Loss after 59000 iterations is :53.0523301333\n",
      "The Smoothed Loss after 60000 iterations is :51.7969492526\n",
      "Printing generated text:\n",
      "ld lartione , ashy gapled foule fsetch sore thoudm sargy.\n",
      "\"Whanchiline ane Hangreffwared neversed sttald of that faest theumea war\n",
      "\"\" mant\" ghming she gacd the bears wis stle gliseing famide and a fold\n",
      "\n",
      "The Smoothed Loss after 61000 iterations is :51.9136524266\n",
      "The Smoothed Loss after 62000 iterations is :51.2031351104\n",
      "The Smoothed Loss after 63000 iterations is :51.7314611665\n",
      "The Smoothed Loss after 64000 iterations is :50.6609719921\n",
      "The Smoothed Loss after 65000 iterations is :50.7676292265\n",
      "The Smoothed Loss after 66000 iterations is :49.8689611656\n",
      "The Smoothed Loss after 67000 iterations is :50.5296718013\n",
      "The Smoothed Loss after 68000 iterations is :51.0710988331\n",
      "The Smoothed Loss after 69000 iterations is :51.1316513798\n",
      "The Smoothed Loss after 70000 iterations is :51.2986015731\n",
      "Printing generated text:\n",
      "lioned hon a,t a d's eouir, matcully hare capbatare bogh\"\n",
      "\" a got hor thacked thoured aunt in yond,\"  wiosing wper us to beem the ind had a berefs vermyen\n",
      "Herm waie with caty ersark montreve unrisel th\n",
      "\n",
      "The Smoothed Loss after 71000 iterations is :51.3586374516\n",
      "The Smoothed Loss after 72000 iterations is :50.6915128454\n",
      "The Smoothed Loss after 73000 iterations is :50.6130254326\n",
      "The Smoothed Loss after 74000 iterations is :50.3529318839\n",
      "The Smoothed Loss after 75000 iterations is :49.655082506\n",
      "The Smoothed Loss after 76000 iterations is :49.8935619325\n",
      "The Smoothed Loss after 77000 iterations is :50.2802557238\n",
      "The Smoothed Loss after 78000 iterations is :49.7985891624\n",
      "The Smoothed Loss after 79000 iterations is :48.8637744102\n",
      "The Smoothed Loss after 80000 iterations is :49.1194724246\n",
      "Printing generated text:\n",
      "lind in flesedon, whan mamuldeed uppackonly intomof wat silating of neul me\" ap - hecl a fowny wasked aboken.  ame sus mor farew hagrent a semenf Hilest al ne wean wibeve an, ragghoume disire, Merard g\n",
      "\n",
      "The Smoothed Loss after 81000 iterations is :48.8979652967\n",
      "The Smoothed Loss after 82000 iterations is :49.1318948009\n",
      "The Smoothed Loss after 83000 iterations is :48.3697791173\n",
      "The Smoothed Loss after 84000 iterations is :49.2815714918\n",
      "The Smoothed Loss after 85000 iterations is :48.1447731774\n",
      "The Smoothed Loss after 86000 iterations is :48.2281143691\n",
      "The Smoothed Loss after 87000 iterations is :48.8749720207\n",
      "The Smoothed Loss after 88000 iterations is :48.3566047967\n",
      "The Smoothed Loss after 89000 iterations is :49.2637075177\n",
      "The Smoothed Loss after 90000 iterations is :49.3697842229\n",
      "Printing generated text:\n",
      "lut that'he, kear ttoutie agrup albets oifusf wholl hadmels soont, asain on of the eursting a . wely hinlDuittopebstang waih in to beetible wempul.  He kifu hassed to anvilmy wase hidlys lee my had sir\n",
      "\n",
      "The Smoothed Loss after 91000 iterations is :49.9130177276\n",
      "The Smoothed Loss after 92000 iterations is :49.8003013784\n",
      "The Smoothed Loss after 93000 iterations is :49.6795412825\n",
      "The Smoothed Loss after 94000 iterations is :50.1069279125\n",
      "The Smoothed Loss after 95000 iterations is :51.0177662146\n",
      "The Smoothed Loss after 96000 iterations is :50.5507754806\n",
      "The Smoothed Loss after 97000 iterations is :48.9574052567\n",
      "The Smoothed Loss after 98000 iterations is :49.6360553765\n",
      "The Smoothed Loss after 99000 iterations is :50.327798752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 100000 iterations is :50.8877773376\n",
      "Printing generated text:\n",
      "lch it five e waideckons as there, gby aurlind thrig theye.\n",
      "\"\"\n",
      "shasp the masied the yourmibord feows, piley therew of theselouserain anco ofow a wy if wsidore sit to noope, acimie look, booke be of aid\n",
      "\n",
      "The Smoothed Loss after 101000 iterations is :50.8599183424\n",
      "The Smoothed Loss after 102000 iterations is :49.7658931326\n",
      "The Smoothed Loss after 103000 iterations is :51.3734288152\n",
      "The Smoothed Loss after 104000 iterations is :49.8312872603\n",
      "The Smoothed Loss after 105000 iterations is :49.1902357459\n",
      "The Smoothed Loss after 106000 iterations is :48.9379218173\n",
      "The Smoothed Loss after 107000 iterations is :49.5234572675\n",
      "The Smoothed Loss after 108000 iterations is :48.7140956539\n",
      "The Smoothed Loss after 109000 iterations is :48.4793126092\n",
      "The Smoothed Loss after 110000 iterations is :47.7708989178\n",
      "Printing generated text:\n",
      "lkon in timkurd and ever whrim and dragdend at sppruting agomed ti. . lot int'rd. . . lout eaci, stemve comton halssbay onft, down teemed lead follom -d.\n",
      "Awis Harry the tee to awasting be to ound had b\n",
      "\n",
      "The Smoothed Loss after 111000 iterations is :48.4630477237\n",
      "The Smoothed Loss after 112000 iterations is :48.7162865611\n",
      "The Smoothed Loss after 113000 iterations is :49.0255613306\n",
      "The Smoothed Loss after 114000 iterations is :48.8018946343\n",
      "The Smoothed Loss after 115000 iterations is :49.2891090193\n",
      "The Smoothed Loss after 116000 iterations is :48.3763101887\n",
      "The Smoothed Loss after 117000 iterations is :48.9096289955\n",
      "The Smoothed Loss after 118000 iterations is :48.6479538658\n",
      "The Smoothed Loss after 119000 iterations is :47.0400666259\n",
      "The Smoothed Loss after 120000 iterations is :47.7415013698\n",
      "Printing generated text:\n",
      "lew the pind  Smmoxcaning, in callfed alded tor tighn and. nandeess anls it on . dinggerew hages bulbetS rise. , . t, Hron sle pe, Harry epe.  \"- and to dors maHarry, onoroe intoaut the bloan and your \n",
      "\n",
      "The Smoothed Loss after 121000 iterations is :47.7973532876\n",
      "The Smoothed Loss after 122000 iterations is :48.4434244568\n",
      "The Smoothed Loss after 123000 iterations is :47.0885949844\n",
      "The Smoothed Loss after 124000 iterations is :46.5822815559\n",
      "The Smoothed Loss after 125000 iterations is :47.1296213616\n",
      "The Smoothed Loss after 126000 iterations is :47.7894862997\n",
      "The Smoothed Loss after 127000 iterations is :47.2170310784\n",
      "The Smoothed Loss after 128000 iterations is :47.4278217637\n",
      "The Smoothed Loss after 129000 iterations is :46.1053000713\n",
      "The Smoothed Loss after 130000 iterations is :46.6973922431\n",
      "Printing generated text:\n",
      "le papt was Cown and then marhp to thidd wendolyars her.  aid som \"uf\"t forving I shol astan pafw my fies.  us waved that of fogsily wand fantrillew pouth has a sot in samiated him biyey her frustlaned\n",
      "\n",
      "The Smoothed Loss after 131000 iterations is :46.213552927\n",
      "The Smoothed Loss after 132000 iterations is :46.6824687247\n",
      "The Smoothed Loss after 133000 iterations is :47.8985689026\n",
      "The Smoothed Loss after 134000 iterations is :47.8714048476\n",
      "The Smoothed Loss after 135000 iterations is :48.2884451727\n",
      "The Smoothed Loss after 136000 iterations is :48.1370366302\n",
      "The Smoothed Loss after 137000 iterations is :48.0760648368\n",
      "The Smoothed Loss after 138000 iterations is :47.9558958268\n",
      "The Smoothed Loss after 139000 iterations is :48.6522918672\n",
      "The Smoothed Loss after 140000 iterations is :50.1137519322\n",
      "Printing generated text:\n",
      "ly strey noteed hadd dfolingers, looHald . . so d meated cantingestley resuse thoughe at thre, he terises.  Harry fee save a said boaly he werey.t lache.  \" lasgeded cound once poult wat up thut soote \n",
      "\n",
      "The Smoothed Loss after 141000 iterations is :47.9586013402\n",
      "The Smoothed Loss after 142000 iterations is :47.9707809824\n",
      "The Smoothed Loss after 143000 iterations is :48.383516204\n",
      "The Smoothed Loss after 144000 iterations is :50.0290124512\n",
      "The Smoothed Loss after 145000 iterations is :49.3386584339\n",
      "The Smoothed Loss after 146000 iterations is :48.5588387009\n",
      "The Smoothed Loss after 147000 iterations is :50.4176458734\n",
      "The Smoothed Loss after 148000 iterations is :48.9589964257\n",
      "The Smoothed Loss after 149000 iterations is :47.5566709488\n",
      "The Smoothed Loss after 150000 iterations is :47.6210306711\n",
      "Printing generated text:\n",
      "lateen,ort he gowe off out if, molrport the,\"  ynusce and emets of room theming.\n",
      "\"\"I ceappors at food in, to hand waHe he whe a hingare baume yould foring on halsed, eyeuld hand of abyethad to nouply h\n",
      "\n",
      "The Smoothed Loss after 151000 iterations is :48.0334205455\n",
      "The Smoothed Loss after 152000 iterations is :47.7346743608\n",
      "The Smoothed Loss after 153000 iterations is :46.9654704379\n",
      "The Smoothed Loss after 154000 iterations is :46.7624458238\n",
      "The Smoothed Loss after 155000 iterations is :46.9854861274\n",
      "The Smoothed Loss after 156000 iterations is :46.8696744891\n",
      "The Smoothed Loss after 157000 iterations is :47.2758270881\n",
      "The Smoothed Loss after 158000 iterations is :47.4637448262\n",
      "The Smoothed Loss after 159000 iterations is :47.811295446\n",
      "The Smoothed Loss after 160000 iterations is :47.4839813136\n",
      "Printing generated text:\n",
      "le of you bay wely anday to iard, sust pelan mestise, retran leacas ofe thry saire.  naked and seculat telly rcove, us he sey I' deat and his as look, Harry als to have bade hat sperbes?\" sluaily and a\n",
      "\n",
      "The Smoothed Loss after 161000 iterations is :47.37304356\n",
      "The Smoothed Loss after 162000 iterations is :47.0701111492\n",
      "The Smoothed Loss after 163000 iterations is :46.4363935478\n",
      "The Smoothed Loss after 164000 iterations is :47.0240266615\n",
      "The Smoothed Loss after 165000 iterations is :46.5377184544\n",
      "The Smoothed Loss after 166000 iterations is :47.0243156972\n",
      "The Smoothed Loss after 167000 iterations is :46.1370594256\n",
      "The Smoothed Loss after 168000 iterations is :46.2425560694\n",
      "The Smoothed Loss after 169000 iterations is :46.1876878212\n",
      "The Smoothed Loss after 170000 iterations is :46.9475423692\n",
      "Printing generated text:\n",
      "lst.  wrea had latcher.\"\n",
      "\"Heboning earts endo latter af him cpunding.  Thaisy Migy brewy atHr o. tpokend thougatar\n",
      "saw, them, and Hermiones, kner bapityoy,\" Hanine his.  tars. Ana cotal gere, liuss sta\n",
      "\n",
      "The Smoothed Loss after 171000 iterations is :46.1897661739\n",
      "The Smoothed Loss after 172000 iterations is :46.0460945688\n",
      "The Smoothed Loss after 173000 iterations is :45.3674185871\n",
      "The Smoothed Loss after 174000 iterations is :45.21549452\n",
      "The Smoothed Loss after 175000 iterations is :45.0800898207\n",
      "The Smoothed Loss after 176000 iterations is :45.6282892993\n",
      "The Smoothed Loss after 177000 iterations is :45.9115239409\n",
      "The Smoothed Loss after 178000 iterations is :47.3148520058\n",
      "The Smoothed Loss after 179000 iterations is :47.2694113462\n",
      "The Smoothed Loss after 180000 iterations is :47.2886504771\n",
      "Printing generated text:\n",
      "liIt very he so coming her.  nar bacheranforgees 't the whilyy've that his thin's dwry,\" soapy, Masst, .\n",
      " a.  rash's loomed his Pooked a facrt at the spon\n",
      "could his cone that him ally seaigh veald was \n",
      "\n",
      "The Smoothed Loss after 181000 iterations is :47.1107023177\n",
      "The Smoothed Loss after 182000 iterations is :47.0242600316\n",
      "The Smoothed Loss after 183000 iterations is :47.7108791176\n",
      "The Smoothed Loss after 184000 iterations is :49.3420876955\n",
      "The Smoothed Loss after 185000 iterations is :47.2600937021\n",
      "The Smoothed Loss after 186000 iterations is :46.9102183335\n",
      "The Smoothed Loss after 187000 iterations is :47.1968129282\n",
      "The Smoothed Loss after 188000 iterations is :49.9949985813\n",
      "The Smoothed Loss after 189000 iterations is :49.2442739606\n",
      "The Smoothed Loss after 190000 iterations is :48.5657732599\n",
      "Printing generated text:\n",
      "le to bece ban's riggraf. Mvested owceroreat.  thry for havill theker rive wriago been a seed poose comtippeus ney a  ack therry asry at Magringed \"ordlar beford his kis nespders barmaniding thiss.  ry\n",
      "\n",
      "The Smoothed Loss after 191000 iterations is :47.361211874\n",
      "The Smoothed Loss after 192000 iterations is :48.5163760697\n",
      "The Smoothed Loss after 193000 iterations is :47.2256460784\n",
      "The Smoothed Loss after 194000 iterations is :47.2846039421\n",
      "The Smoothed Loss after 195000 iterations is :46.5767057953\n",
      "The Smoothed Loss after 196000 iterations is :47.5490176716\n",
      "The Smoothed Loss after 197000 iterations is :46.2284814455\n",
      "The Smoothed Loss after 198000 iterations is :46.1672966544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 199000 iterations is :45.753016386\n",
      "The Smoothed Loss after 200000 iterations is :46.525339471\n",
      "Printing generated text:\n",
      "linf ither, aobed and younws the Dainter, teed apamthely some.\n",
      "\"Haswhe wass, soints was neew, wiiw ssied so a larce frims mones stolchiven of whos.  tick  Trenalesest.\n",
      "\"nan t so walt, not the hert liki\n",
      "\n",
      "The Smoothed Loss after 201000 iterations is :46.7302720959\n",
      "The Smoothed Loss after 202000 iterations is :46.9537535297\n",
      "The Smoothed Loss after 203000 iterations is :47.1075935494\n",
      "The Smoothed Loss after 204000 iterations is :46.8910688573\n",
      "The Smoothed Loss after 205000 iterations is :46.733926388\n",
      "The Smoothed Loss after 206000 iterations is :46.4562941881\n",
      "The Smoothed Loss after 207000 iterations is :46.4923684796\n",
      "The Smoothed Loss after 208000 iterations is :46.1673072116\n",
      "The Smoothed Loss after 209000 iterations is :46.0269771562\n",
      "The Smoothed Loss after 210000 iterations is :46.6430572356\n",
      "Printing generated text:\n",
      "llowet to ranal Mous now'd upfortred to each.  He lood her lomking in.  \"y'd de'dp ald yow gain he's cark into concw out fred contied, and if nighf goll to cwauck opet srotericill. . .\"\n",
      "\"ed'let, errely\n",
      "\n",
      "The Smoothed Loss after 211000 iterations is :45.6549222257\n",
      "The Smoothed Loss after 212000 iterations is :44.9170694339\n",
      "The Smoothed Loss after 213000 iterations is :45.5789444846\n",
      "The Smoothed Loss after 214000 iterations is :45.2575202837\n",
      "The Smoothed Loss after 215000 iterations is :45.5266621989\n",
      "The Smoothed Loss after 216000 iterations is :44.8069101528\n",
      "The Smoothed Loss after 217000 iterations is :45.663831731\n",
      "The Smoothed Loss after 218000 iterations is :44.3703323055\n",
      "The Smoothed Loss after 219000 iterations is :44.7200635675\n",
      "The Smoothed Loss after 220000 iterations is :44.9111320365\n",
      "Printing generated text:\n",
      "ld scrother thess gomelt ils.  dle oftere.  that the way to the far oint.\"\n",
      "\"Harry no of off werpeno-dans the raneed streared, it arm, os Dedst whil?\", meared so croving was at the arry, spuaring it hav\n",
      "\n",
      "The Smoothed Loss after 221000 iterations is :45.0272761432\n",
      "The Smoothed Loss after 222000 iterations is :46.1516880932\n",
      "The Smoothed Loss after 223000 iterations is :45.6220378539\n",
      "The Smoothed Loss after 224000 iterations is :46.6100328846\n",
      "The Smoothed Loss after 225000 iterations is :46.1755790029\n",
      "The Smoothed Loss after 226000 iterations is :46.3905760593\n",
      "The Smoothed Loss after 227000 iterations is :46.6976503698\n",
      "The Smoothed Loss after 228000 iterations is :48.2255775695\n",
      "The Smoothed Loss after 229000 iterations is :47.0982203137\n",
      "The Smoothed Loss after 230000 iterations is :45.3784155249\n",
      "Printing generated text:\n",
      "lette., and chingHery.\n",
      "\"\n",
      "\" teledouts was non'ling you \"ern and ait his had get Mament of store nes intort.\"\n",
      "\"Werec-pas they plongefort fromed aced to thang Andented and darely't.  Afters they swowes, a\n",
      "\n",
      "The Smoothed Loss after 231000 iterations is :46.317167377\n",
      "The Smoothed Loss after 232000 iterations is :47.1107896401\n",
      "The Smoothed Loss after 233000 iterations is :47.775864476\n",
      "The Smoothed Loss after 234000 iterations is :48.2101958051\n",
      "The Smoothed Loss after 235000 iterations is :46.577410161\n",
      "The Smoothed Loss after 236000 iterations is :48.3661849603\n",
      "The Smoothed Loss after 237000 iterations is :46.4299032759\n",
      "The Smoothed Loss after 238000 iterations is :46.2664588719\n",
      "The Smoothed Loss after 239000 iterations is :45.6864027308\n",
      "The Smoothed Loss after 240000 iterations is :46.4036757272\n",
      "Printing generated text:\n",
      "lon champed to a dary know of sham and Harry. \"hast Rehan tle be setered to lunk eyes, the lave me and eyem, some, very stompered Harry veered helog, had to let, \"erked forte the birpled clince to cher\n",
      "\n",
      "The Smoothed Loss after 241000 iterations is :45.464788868\n",
      "The Smoothed Loss after 242000 iterations is :45.4693765998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 243000 iterations is :inf\n",
      "The Smoothed Loss after 244000 iterations is :inf\n",
      "The Smoothed Loss after 245000 iterations is :inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 246000 iterations is :nan\n",
      "The Smoothed Loss after 247000 iterations is :nan\n",
      "The Smoothed Loss after 248000 iterations is :nan\n",
      "The Smoothed Loss after 249000 iterations is :nan\n",
      "The Smoothed Loss after 250000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:20: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 251000 iterations is :nan\n",
      "The Smoothed Loss after 252000 iterations is :nan\n",
      "The Smoothed Loss after 253000 iterations is :nan\n",
      "The Smoothed Loss after 254000 iterations is :nan\n",
      "The Smoothed Loss after 255000 iterations is :nan\n",
      "The Smoothed Loss after 256000 iterations is :nan\n",
      "The Smoothed Loss after 257000 iterations is :nan\n",
      "The Smoothed Loss after 258000 iterations is :nan\n",
      "The Smoothed Loss after 259000 iterations is :nan\n",
      "The Smoothed Loss after 260000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 261000 iterations is :nan\n",
      "The Smoothed Loss after 262000 iterations is :nan\n",
      "The Smoothed Loss after 263000 iterations is :nan\n",
      "The Smoothed Loss after 264000 iterations is :nan\n",
      "The Smoothed Loss after 265000 iterations is :nan\n",
      "The Smoothed Loss after 266000 iterations is :nan\n",
      "The Smoothed Loss after 267000 iterations is :nan\n",
      "The Smoothed Loss after 268000 iterations is :nan\n",
      "The Smoothed Loss after 269000 iterations is :nan\n",
      "The Smoothed Loss after 270000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 271000 iterations is :nan\n",
      "The Smoothed Loss after 272000 iterations is :nan\n",
      "The Smoothed Loss after 273000 iterations is :nan\n",
      "The Smoothed Loss after 274000 iterations is :nan\n",
      "The Smoothed Loss after 275000 iterations is :nan\n",
      "The Smoothed Loss after 276000 iterations is :nan\n",
      "The Smoothed Loss after 277000 iterations is :nan\n",
      "The Smoothed Loss after 278000 iterations is :nan\n",
      "The Smoothed Loss after 279000 iterations is :nan\n",
      "The Smoothed Loss after 280000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 281000 iterations is :nan\n",
      "The Smoothed Loss after 282000 iterations is :nan\n",
      "The Smoothed Loss after 283000 iterations is :nan\n",
      "The Smoothed Loss after 284000 iterations is :nan\n",
      "The Smoothed Loss after 285000 iterations is :nan\n",
      "The Smoothed Loss after 286000 iterations is :nan\n",
      "The Smoothed Loss after 287000 iterations is :nan\n",
      "The Smoothed Loss after 288000 iterations is :nan\n",
      "The Smoothed Loss after 289000 iterations is :nan\n",
      "The Smoothed Loss after 290000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 291000 iterations is :nan\n",
      "The Smoothed Loss after 292000 iterations is :nan\n",
      "The Smoothed Loss after 293000 iterations is :nan\n",
      "The Smoothed Loss after 294000 iterations is :nan\n",
      "The Smoothed Loss after 295000 iterations is :nan\n",
      "The Smoothed Loss after 296000 iterations is :nan\n",
      "The Smoothed Loss after 297000 iterations is :nan\n",
      "The Smoothed Loss after 298000 iterations is :nan\n",
      "The Smoothed Loss after 299000 iterations is :nan\n",
      "The Smoothed Loss after 300000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 301000 iterations is :nan\n",
      "The Smoothed Loss after 302000 iterations is :nan\n",
      "The Smoothed Loss after 303000 iterations is :nan\n",
      "The Smoothed Loss after 304000 iterations is :nan\n",
      "The Smoothed Loss after 305000 iterations is :nan\n",
      "The Smoothed Loss after 306000 iterations is :nan\n",
      "The Smoothed Loss after 307000 iterations is :nan\n",
      "The Smoothed Loss after 308000 iterations is :nan\n",
      "The Smoothed Loss after 309000 iterations is :nan\n",
      "The Smoothed Loss after 310000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 311000 iterations is :nan\n",
      "The Smoothed Loss after 312000 iterations is :nan\n",
      "The Smoothed Loss after 313000 iterations is :nan\n",
      "The Smoothed Loss after 314000 iterations is :nan\n",
      "The Smoothed Loss after 315000 iterations is :nan\n",
      "The Smoothed Loss after 316000 iterations is :nan\n",
      "The Smoothed Loss after 317000 iterations is :nan\n",
      "The Smoothed Loss after 318000 iterations is :nan\n",
      "The Smoothed Loss after 319000 iterations is :nan\n",
      "The Smoothed Loss after 320000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 321000 iterations is :nan\n",
      "The Smoothed Loss after 322000 iterations is :nan\n",
      "The Smoothed Loss after 323000 iterations is :nan\n",
      "The Smoothed Loss after 324000 iterations is :nan\n",
      "The Smoothed Loss after 325000 iterations is :nan\n",
      "The Smoothed Loss after 326000 iterations is :nan\n",
      "The Smoothed Loss after 327000 iterations is :nan\n",
      "The Smoothed Loss after 328000 iterations is :nan\n",
      "The Smoothed Loss after 329000 iterations is :nan\n",
      "The Smoothed Loss after 330000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 331000 iterations is :nan\n",
      "The Smoothed Loss after 332000 iterations is :nan\n",
      "The Smoothed Loss after 333000 iterations is :nan\n",
      "The Smoothed Loss after 334000 iterations is :nan\n",
      "The Smoothed Loss after 335000 iterations is :nan\n",
      "The Smoothed Loss after 336000 iterations is :nan\n",
      "The Smoothed Loss after 337000 iterations is :nan\n",
      "The Smoothed Loss after 338000 iterations is :nan\n",
      "The Smoothed Loss after 339000 iterations is :nan\n",
      "The Smoothed Loss after 340000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 341000 iterations is :nan\n",
      "The Smoothed Loss after 342000 iterations is :nan\n",
      "The Smoothed Loss after 343000 iterations is :nan\n",
      "The Smoothed Loss after 344000 iterations is :nan\n",
      "The Smoothed Loss after 345000 iterations is :nan\n",
      "The Smoothed Loss after 346000 iterations is :nan\n",
      "The Smoothed Loss after 347000 iterations is :nan\n",
      "The Smoothed Loss after 348000 iterations is :nan\n",
      "The Smoothed Loss after 349000 iterations is :nan\n",
      "The Smoothed Loss after 350000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 351000 iterations is :nan\n",
      "The Smoothed Loss after 352000 iterations is :nan\n",
      "The Smoothed Loss after 353000 iterations is :nan\n",
      "The Smoothed Loss after 354000 iterations is :nan\n",
      "The Smoothed Loss after 355000 iterations is :nan\n",
      "The Smoothed Loss after 356000 iterations is :nan\n",
      "The Smoothed Loss after 357000 iterations is :nan\n",
      "The Smoothed Loss after 358000 iterations is :nan\n",
      "The Smoothed Loss after 359000 iterations is :nan\n",
      "The Smoothed Loss after 360000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 361000 iterations is :nan\n",
      "The Smoothed Loss after 362000 iterations is :nan\n",
      "The Smoothed Loss after 363000 iterations is :nan\n",
      "The Smoothed Loss after 364000 iterations is :nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 365000 iterations is :nan\n",
      "The Smoothed Loss after 366000 iterations is :nan\n",
      "The Smoothed Loss after 367000 iterations is :nan\n",
      "The Smoothed Loss after 368000 iterations is :nan\n",
      "The Smoothed Loss after 369000 iterations is :nan\n",
      "The Smoothed Loss after 370000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 371000 iterations is :nan\n",
      "The Smoothed Loss after 372000 iterations is :nan\n",
      "The Smoothed Loss after 373000 iterations is :nan\n",
      "The Smoothed Loss after 374000 iterations is :nan\n",
      "The Smoothed Loss after 375000 iterations is :nan\n",
      "The Smoothed Loss after 376000 iterations is :nan\n",
      "The Smoothed Loss after 377000 iterations is :nan\n",
      "The Smoothed Loss after 378000 iterations is :nan\n",
      "The Smoothed Loss after 379000 iterations is :nan\n",
      "The Smoothed Loss after 380000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 381000 iterations is :nan\n",
      "The Smoothed Loss after 382000 iterations is :nan\n",
      "The Smoothed Loss after 383000 iterations is :nan\n",
      "The Smoothed Loss after 384000 iterations is :nan\n",
      "The Smoothed Loss after 385000 iterations is :nan\n",
      "The Smoothed Loss after 386000 iterations is :nan\n",
      "The Smoothed Loss after 387000 iterations is :nan\n",
      "The Smoothed Loss after 388000 iterations is :nan\n",
      "The Smoothed Loss after 389000 iterations is :nan\n",
      "The Smoothed Loss after 390000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 391000 iterations is :nan\n",
      "The Smoothed Loss after 392000 iterations is :nan\n",
      "The Smoothed Loss after 393000 iterations is :nan\n",
      "The Smoothed Loss after 394000 iterations is :nan\n",
      "The Smoothed Loss after 395000 iterations is :nan\n",
      "The Smoothed Loss after 396000 iterations is :nan\n",
      "The Smoothed Loss after 397000 iterations is :nan\n",
      "The Smoothed Loss after 398000 iterations is :nan\n",
      "The Smoothed Loss after 399000 iterations is :nan\n",
      "The Smoothed Loss after 400000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 401000 iterations is :nan\n",
      "The Smoothed Loss after 402000 iterations is :nan\n",
      "The Smoothed Loss after 403000 iterations is :nan\n",
      "The Smoothed Loss after 404000 iterations is :nan\n",
      "The Smoothed Loss after 405000 iterations is :nan\n",
      "The Smoothed Loss after 406000 iterations is :nan\n",
      "The Smoothed Loss after 407000 iterations is :nan\n",
      "The Smoothed Loss after 408000 iterations is :nan\n",
      "The Smoothed Loss after 409000 iterations is :nan\n",
      "The Smoothed Loss after 410000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 411000 iterations is :nan\n",
      "The Smoothed Loss after 412000 iterations is :nan\n",
      "The Smoothed Loss after 413000 iterations is :nan\n",
      "The Smoothed Loss after 414000 iterations is :nan\n",
      "The Smoothed Loss after 415000 iterations is :nan\n",
      "The Smoothed Loss after 416000 iterations is :nan\n",
      "The Smoothed Loss after 417000 iterations is :nan\n",
      "The Smoothed Loss after 418000 iterations is :nan\n",
      "The Smoothed Loss after 419000 iterations is :nan\n",
      "The Smoothed Loss after 420000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 421000 iterations is :nan\n",
      "The Smoothed Loss after 422000 iterations is :nan\n",
      "The Smoothed Loss after 423000 iterations is :nan\n",
      "The Smoothed Loss after 424000 iterations is :nan\n",
      "The Smoothed Loss after 425000 iterations is :nan\n",
      "The Smoothed Loss after 426000 iterations is :nan\n",
      "The Smoothed Loss after 427000 iterations is :nan\n",
      "The Smoothed Loss after 428000 iterations is :nan\n",
      "The Smoothed Loss after 429000 iterations is :nan\n",
      "The Smoothed Loss after 430000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 431000 iterations is :nan\n",
      "The Smoothed Loss after 432000 iterations is :nan\n",
      "The Smoothed Loss after 433000 iterations is :nan\n",
      "The Smoothed Loss after 434000 iterations is :nan\n",
      "The Smoothed Loss after 435000 iterations is :nan\n",
      "The Smoothed Loss after 436000 iterations is :nan\n",
      "The Smoothed Loss after 437000 iterations is :nan\n",
      "The Smoothed Loss after 438000 iterations is :nan\n",
      "The Smoothed Loss after 439000 iterations is :nan\n",
      "The Smoothed Loss after 440000 iterations is :nan\n",
      "Printing generated text:\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "The Smoothed Loss after 441000 iterations is :nan\n",
      "The Smoothed Loss after 442000 iterations is :nan\n",
      "The Smoothed Loss after 443000 iterations is :nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmclXX5//HXNQv7DgMioAOCIqa4jAvuCm5gaWVqWWJZ\nZj/NsqxG+bqVKKlpLpVZLmiluWRQuAHmviAqLoDINij7sO8wzFy/P+77nDmznJlhzjqc9/PxmMfc\n53Puc9/XZw6c63yW+3ObuyMiItJceZkOQEREWjYlEhERSYgSiYiIJESJREREEqJEIiIiCVEiERGR\nhCiRiOxGzGyTmQ3IdBySW5RIJCuYWZmZjUjzOY8ys81m1qGe5z4ws8t38XjROpjZRWb2erJijXO+\nl83s+7Fl7t7B3Rek8rwitSmRSM5y97eBxcA5seVm9iVgCPBYJuIKYyjI1LlFdpUSiWQ9M/uBmc0z\nszVmNtHM9gzLzczuNLOVZrbBzD4OkwBmNtLMZpnZRjNbYmZXxTn8eODCWmUXAs+6+2oza2NmfzOz\n1Wa2zszeNbNejcS7P3AfMCzsaloXlrc2s9vN7HMzW2Fm95lZ2/C5E81ssZn9ysyWAw+ZWVcz+6+Z\nlZvZ2nC7b7j/WOA44N7wHPeG5W5mA8Ptzmb2SPj6RWb2f2aWFz53kZm9Hsaz1swWmtkZMXW4yMwW\nhH+/hWZ2QZPfMMk5SiSS1czsZOAW4FygN7AIeDx8+lTgeGBfoHO4z+rwuQeAH7p7R+BLwEtxTvEo\ncLyZ9QvPlwd8iyDBAIwOj90P6A5cCmxtKGZ3nx3u91bY1dQlfGpcGOvBwECgD3BdzEv3ALoBewOX\nEPz/fCh8vFd43nvDc4wBXgMuD89RXzfcPWHsA4ATCBLkd2OePxKYA/QAbgUeCJNze+Bu4Izw73c0\nMKOhOktuUyKRbHcB8KC7v+/u24GrCb7pFwMVQEdgMGDuPtvdl4WvqwCGmFknd1/r7u/Xd3B3/wJ4\nGfhOWDQcaA1MijlOd2Cgu1e6+3vuvmFXK2FmRpAcrnT3Ne6+EbgZOD9mtyrgenff7u5b3X21uz/t\n7lvC/ccSJISmnC8/PPbV7r7R3cuA38XUE2CRu//F3SsJEmdvINLaqgK+ZGZt3X2Zu8/c1TpL7lAi\nkWy3J0ErBAB330TQ6ujj7i8RfEP/A7DSzO43s07hrl8HRgKLzOwVMxvWwDnGU/0B+x3gcXevCB8/\nCrwAPG5mS83sVjMrbEY9ioB2wHthF9k64PmwPKLc3bdFHphZOzP7c9gttQF4FegSJonG9AAKifnb\nhdt9Yh4vj2y4+5Zws4O7bwbOI2hVLTOzSWY2uMk1lZyjRCLZbilB1w4AYbdLd2AJgLvf7e6HEQyO\n7wv8Iix/193PAnoC/waeaOAc/wL6mtlJwNeo7tbC3Svc/UZ3H0LQxXMmdcdU6lN7We1VBF1TB7h7\nl/Cns7t3aOA1Pwf2A450904E3XgAFmf/2uerIOZvR9A9tqQJsePuL7j7KQStlE+BvzTldZKblEgk\nmxSGg9uRnwKCmVPfNbODzaw1QXfQO+5eZmaHm9mRYQthM7ANqDKzVmZ2gZl1DlsWGwi6auoVfgN/\nimA8YpG7T488Z2YnmdmBYStgA8GHc9xjxVhBkJxaheeoIvgwvtPMeobH7mNmpzVwjI4EyWedmXUD\nrq/nHPVeMxJ2Vz0BjDWzjma2N/Az4G+NBW5mvczsrDBpbwc20bQ6S45SIpFs8izBB2fk5wZ3nwJc\nCzwNLAP2oXpcoRPBh/Nagm6b1cBt4XPfAcrCLqFLCcZaGjKe4Nv7I7XK9yBIMhuA2cArBN1djXkJ\nmAksN7NVYdmvgHnA22FcUwhaHPH8HmhL0Lp4m6ArLNZdwDnhrKu763n9jwkS7ALgdeAfwINNiD2P\nIOksBdYQjMv8qAmvkxxlurGViIgkQi0SERFJiBKJiIgkRIlEREQSokQiIiIJadELw/Xo0cOLi4sz\nHYaISIvy3nvvrXL3osb3bJoWnUiKi4uZPn164zuKiEiUmS1qfK+mU9eWiIgkRIlEREQSokQiIiIJ\nUSIREZGEpCyRmNmD4Z3rPokp+4aZzTSzKjMrqbX/1RbcBW9OIwvZiYhIFklli+Rh4PRaZZ8QLNP9\namyhmQ0hWIjvgPA1f2ziPRdERCTDUpZI3P1VgpVDY8tmu/ucenY/i+BmQtvdfSHBCqlHpCo2ERFJ\nnmwZI+kDfBHzeDE17+QWZWaXmNl0M5teXl7erJMtWbeVO16cQ9mqzc16vYiIVMuWRNJk7n6/u5e4\ne0lRUfMuzFy1cTt3vzSPeSs3JTk6EZHcky2JZAnQL+ZxX5p4S9DmKMwPql1RqZu+iYgkKlsSyUTg\nfDNrbWb9gUHAtFSdrDA/uOX1zird1EtEJFEpW2vLzB4DTgR6mNligvtNrwHuAYqASWY2w91Pc/eZ\nZvYEMAvYCVwW3nM6JQrCFsnOKrVIREQSlbJE4u7fjPPUM3H2HwuMTVU8sQryghZJRaVaJCIiicqW\nrq20Kgi7tirVtSUikrDcTCR5YdeWBttFRBKWk4kkMtiuri0RkcTlZCLJz1PXlohIsuRkIoleR6JZ\nWyIiCcvJRBKZtbVTXVsiIgnLyUQS6drSBYkiIonLyURiZhTkmWZtiYgkQU4mEgiuJVGLREQkcTmb\nSArz8rRoo4hIEuRsIsnPN03/FRFJgpxNJAV5ebogUUQkCXI2kRTma7BdRCQZcjaR5Oepa0tEJBly\nNpEU5udRoUQiIpKwnE0kuo5ERCQ5cjaR5OfpOhIRkWTI2URSmJ+nFomISBLkbCLRle0iIsmRs4lE\nV7aLiCRHziYSTf8VEUmOlCUSM3vQzFaa2ScxZd3MbLKZzQ1/d4157mozm2dmc8zstFTFFVGQb7qy\nXUQkCVLZInkYOL1WWSkw1d0HAVPDx5jZEOB84IDwNX80s/wUxhYMtusOiSIiCUtZInH3V4E1tYrP\nAsaH2+OBs2PKH3f37e6+EJgHHJGq2CCc/qsWiYhIwtI9RtLL3ZeF28uBXuF2H+CLmP0Wh2V1mNkl\nZjbdzKaXl5c3O5BCzdoSEUmKjA22u7sDu/xJ7u73u3uJu5cUFRU1+/wFebqOREQkGdKdSFaYWW+A\n8PfKsHwJ0C9mv75hWcoU6Mp2EZGkSHcimQiMDrdHAxNiys83s9Zm1h8YBExLZSAF+RojERFJhoJU\nHdjMHgNOBHqY2WLgemAc8ISZXQwsAs4FcPeZZvYEMAvYCVzm7pWpig2gQLO2RESSImWJxN2/Geep\n4XH2HwuMTVU8tRWqa0tEJCly+Mr2PHVtiYgkQc4mksJ801pbIiJJkLOJRKv/iogkR84mkvy8PCqr\nnOByFhERaa6cTSSFeQagVomISIJyNpEU5AdV14C7iEhicjaRFOYHLZIKXUsiIpKQHE4kapGIiCRD\nzieSHTvVIhERSUTOJpJWBUokIiLJkLOJpHUkkVSmdEkvEZHdXs4mkkiLZFuFWiQiIonI2URS3SJR\nIhERSUQOJ5J8ALarRSIikpDcTSSFQdW379QYiYhIInI2kbTS9F8RkaTI2UTSJtoiUSIREUlEziaS\n6BiJEomISEJyNpHogkQRkeTI2UQSmf6rwXYRkcTkcCIJurZ0QaKISGIykkjM7Cdm9omZzTSzn4Zl\n3cxsspnNDX93TWUMraNXtqtFIiKSiLQnEjP7EvAD4AhgKHCmmQ0ESoGp7j4ImBo+Tpm8PKN1QR7b\n1LUlIpKQTLRI9gfecfct7r4TeAX4GnAWMD7cZzxwdqoDaVOYz7YdSiQiIonIRCL5BDjOzLqbWTtg\nJNAP6OXuy8J9lgO96nuxmV1iZtPNbHp5eXlCgbQtzNcYiYhIgtKeSNx9NvBb4EXgeWAGUFlrHwfq\nvXWhu9/v7iXuXlJUVJRQLG0K89iqMRIRkYRkZLDd3R9w98Pc/XhgLfAZsMLMegOEv1emOo42hfka\nbBcRSVCmZm31DH/vRTA+8g9gIjA63GU0MCHVcXy6fCMvzlqR6tOIiOzWCjJ03qfNrDtQAVzm7uvM\nbBzwhJldDCwCzs1QbCIisgsykkjc/bh6ylYDw9MZx1eG7slHi9el85QiIrudnL2yHaBLu0LWba3I\ndBgiIi1abieStoWs31pBVVW9E8RERKQJcjqRdG7XCnfYuG1npkMREWmxcjqRdGlbCMC6rTsyHImI\nSMuV04mkc5hI1mucRESk2XI6kXRpF7ZItiiRiIg0lxIJaOaWiEgCcjqRdG7bCoD1WzRGIiLSXDme\nSNS1JSKSqJxOJK0K8mjXKl+D7SIiCcjpRAKwZUclz3ywJNNhiIi0WDmfSABWb9YYiYhIcymRiIhI\nQnI+kXztkD6ZDkFEpEXL+UTyr3B8RAPuIiLNk/OJ5FtH7gXAvJWbMhyJiEjLlPOJ5IjibgBMma1b\n7oqINEfOJ5KD+nYG4E8vz89wJCIiLVPOJ5K9u7fPdAgiIi1azieS/DyjuHu7TIchItJi5XwiAShb\nvSXTIYiItFgZSSRmdqWZzTSzT8zsMTNrY2bdzGyymc0Nf3dNd1xlqzan+5QiIi1e2hOJmfUBrgBK\n3P1LQD5wPlAKTHX3QcDU8HFanFfSD9BSKSIizZGprq0CoK2ZFQDtgKXAWcD48PnxwNnpCuackr4A\nPP/JsnSdUkRkt5H2ROLuS4Dbgc+BZcB6d38R6OXukU/y5UCv+l5vZpeY2XQzm15eXp6UmPbv3QmA\nRRorERHZZZno2upK0ProD+wJtDezb8fu4+4OeH2vd/f73b3E3UuKioqSElOH1gUAvDhLFyWKiOyq\nJiUSM9vHzFqH2yea2RVm1qWZ5xwBLHT3cnevAP4FHA2sMLPe4Tl6AyubefxmG7xHx3SfUkSkxWtq\ni+RpoNLMBgL3A/2AfzTznJ8DR5lZOzMzYDgwG5gIjA73GQ1MaObxm+W4QT1oU5ifzlOKiOwWCpq4\nX5W77zSzrwL3uPs9ZvZBc07o7u+Y2VPA+8BO4AOC5NQBeMLMLgYWAec25/jN1b19K42RiIg0Q1MT\nSYWZfZOgpfDlsKywuSd19+uB62sVbydonWTEG/NXU75xe6ZOLyLSYjW1a+u7wDBgrLsvNLP+wKOp\nCyv9DuwTLN64cuO2DEciItKyNCmRuPssd7/C3R8LZ111dPffpji2tKqorALgnQVrMhyJiEjL0tRZ\nWy+bWScz60YwtvEXM7sjtaGl15Wn7AtUTwUWEZGmaWrXVmd33wB8DXjE3Y8kmMa72yjq0BpQ15aI\nyK5qaiIpCK/tOBf4bwrjyZiencJEskED7iIiu6KpieTXwAvAfHd/18wGAHNTF1b6tS7Ip2u7Qlao\nRSIiskuaNCDg7k8CT8Y8XgB8PVVBZUqvTm1YoRaJiMguaepge18ze8bMVoY/T5tZ31QHl249O7Vh\n5Qa1SEREdkVTu7YeIljCZM/w5z9h2W6lV8fWapGIiOyipiaSInd/yN13hj8PA8lZejeL9OrUhvJN\n26msqnfhYRERqUdTE8lqM/u2meWHP98GVqcysEzo1ak1lVXO6s1qlYiINFVTE8n3CKb+Lie4GdU5\nwEUpiiljenZqA2gKsIjIrmjqEimL3P0r7l7k7j3d/Wx2w1lbPTq0AuCVz5Jz50URkVyQyB0Sf5a0\nKLJE5H4kf3ltQYYjERFpORJJJJa0KLLE4D2Ce7ev21KR4UhERFqORBLJbje1KT8vyI1fGbpnhiMR\nEWk5GkwkZrbRzDbU87OR4HqS3dLED5dmOgQRkRajwSVS3L1jugLJNlVVTl7ebtd7JyKSdIl0be2W\nIndKXLRG928XEWkKJZJarhg+CIB5KzdlOBIRkZZBiaSWbu2Da0k+/GJdhiMREWkZ0p5IzGw/M5sR\n87PBzH5qZt3MbLKZzQ1/d013bACD9wiGhdrrlrsiIk2S9kTi7nPc/WB3Pxg4DNgCPAOUAlPdfRAw\nNXycdpEE8vKclZk4vYhIi5Pprq3hBHddXAScBYwPy8cDZ2csKuCdhWsyeXoRkRYj04nkfOCxcLuX\nuy8Lt5cDvep7gZldYmbTzWx6eXlq1sTqFd6/XUREGpexRGJmrYCvEHML3wh3d+JcOe/u97t7ibuX\nFBWl5pYom7btjJwrJccXEdmdZLJFcgbwvruvCB+vMLPeAOHvjA1SjDqoNwDvLVqbqRBERFqMTCaS\nb1LdrQXBrXxHh9ujgQlpjyj0xPTFAJxz31uZCkFEpMXISCIxs/bAKcC/YorHAaeY2VxgRPg4I6Zd\nMzxTpxYRaXEycrGEu28GutcqW00wiyvjIndKFBGRxmV61paIiLRwSiSN2FlZlekQRESymhJJI6bM\n1hXuIiINUSKJ4/ZvDAWgqKMuThQRaYgSSRwDe3YAYN2WHRmOREQkuymRxNG1XSEAa7dUZDgSEZHs\npkQSR+S+JFc9+WGGIxERyW5KJHF00P1IRESaRIkkDjPLdAgiIi2CEkkDvndMfwCqqrQKsIhIPEok\nDejfox0AqzZtz3AkIiLZS4mkATvDlsiDb5RlNhARkSymRNKAEfsHN2m875X5AHy+egt/+N+8TIYk\nIpJ1NDWpAf26tavx+Pjb/gfA94/rT+uC/EyEJCKSddQiacSBfToDsH1nZbRs8dqtmQpHRCTrKJE0\n4uMl6wHY7/+ej5YN/90rmQpHRCTrKJE04qKjizMdgohIVlMiacQNXzkg0yGIiGQ1JZJm0rUlIiIB\nJZIm+NMFh9YpW6IBdxERQImkSc44sHedsqmzV2QgEhGR7JORRGJmXczsKTP71Mxmm9kwM+tmZpPN\nbG74u2smYotn/s0jazy++yVdmCgiAplrkdwFPO/ug4GhwGygFJjq7oOAqeHjrJGfZzx68RG8UXpy\ntGz9Vt30SkQk7YnEzDoDxwMPALj7DndfB5wFjA93Gw+cne7YGnPcoCL6dGkbfXz1vz7KYDQiItkh\nEy2S/kA58JCZfWBmfzWz9kAvd18W7rMc6JWB2Jpk1q9PA+DZj5dnOBIRkczLRCIpAA4F/uTuhwCb\nqdWN5e4O1HsTEDO7xMymm9n08vLylAdbn3atqpcoW7Zes7dEJLdlIpEsBha7+zvh46cIEssKM+sN\nEP5eWd+L3f1+dy9x95KioqK0BNyQYbe8xOert2Q6DBGRjEl7InH35cAXZrZfWDQcmAVMBEaHZaOB\nCemObVccv291EpusqcAiksMyNWvrx8Dfzewj4GDgZmAccIqZzQVGhI+z1oOjS6Lbv/nvrAxGIiKS\nWRm5H4m7zwBK6nlqeLpjaa6C/Jo52N0xswxFIyKSObqyPQFPXTosun33VF2gKCK5SYkkASXF3Xjs\nB0cBcOeUzygunaSBdxHJOUokCRq2T/cajyO34xURyRVKJClwx+TPMh2CiEjaKJEkwcJbai3oOHUu\n9740N0PRiIiklxJJEpgZ/77sGP528ZHRsttfVKtERHKDEkmSHNyvC8cO6sGM606Jlk2YsSSDEYmI\npIcSSZJ1adeKI/p3A+Anj8+guHQSd8YZM3nqvcV8sUazvESkZVMiSYEnfjisxuO7plaPl7g7Hy9e\nz/L127jqyQ857lbN8hKRli0jV7bnouLSSXGfm7l0PQfs2TmN0YiIJI9aJCmy8JaRXHDkXk3ad9Td\nrzeYaNLlg8/Xsq2iMtNhiEgLo0SSImbG2K8eyOxfn97k1xSXTmJnZVUKowosW7+V4tJJ/PKpD6Nl\nazbv4Kt/fJNv3PdWys8vIrsXdW2lWNtW+Uy7ZjhtW+XToXUBM75YR89ObaK37K3dEhk45jnKxo1K\naUzDbnkJgCemL+bWc4YCQfcawMdL1kf3Ky6dRMfWBXx842kpjUdEWja1SNKgZ6c2dGxTiJlxyF5d\na9z3vfbFjG0K89hZWYW7U1w6ieLSSVRW1XuzyKRqFbOa8R0vzqEqPOfG7TtTfm4RadmUSDLMzCgb\nN4p5Y8+ga7tCtlVUMXDMc/S/+tnoPvtc82wDR9g1tacbB3c1hry86iXw735pHgNizvlu2ZqknV9E\ndj9KJFmiID+P/fbomPTj3jk5WJX4jhfnMHvZhjrTjR9/9wuABgfZNW4iIg1RIskiby+I/80/0r0V\n6e566dPGb++7bP3W6DUsd780jzPueq3OPlf/62MAvvPAtOaE3GKs31LBlFm6JbJIKiiRZJHLTxoY\n3b7o6OIag+77XPNsjYH57z08vcFj/enl+dFB9fosuLl6bCb2uPd88xD+b9T+dfZfs3lHdLuqyqNd\nYtmqotbst2HjpvL9R6azdN3WuPuISPMokWSRy08OEsm93zqEG75yAABP/2hY3P0jrZPaH4gbtlXw\n2+c/jfu6v3//yBpjIrGG79+Ti4/tX6f80N9Mprh0Egfe8AIDrnmWyx/7oMG6rNq0neLSSRlJOF++\n53UGjXmODz5fGy3bsiPoupu+KCh76r3FDBrzHE+EXXsi0nxKJFmkTWE+ZeNGceZBe0bLDtu7W6Ov\n+9kTH9Z4fGGtbqqycaP48LpTWXjLSMrGjeKYgT3qPc7csWfQrlVBg/ee37gtmMU16aNlcfdxd0pu\nmgLA9RNnNhp/skWmMH/1j2/WeW7q7BXsrKzio8XrAPjl0x+lNTaR3ZESSQsQ2w0FcMXwQTUe/+fD\npbz/+VoWrd7MX19bwIwv1tU5Rud2hXUSxLyxZ0S3F94yksKYKcALbh7JeSX9mF/r3LFiB+g/Xb4h\n2kKKnXH2yFuLGqld6gyOmbxw+gF7ADBhxlIGjnmOgT07ZCqslJi5dD1lqzZnOgzJUbogsQXIyzNG\nHdibtVt28I/wHvF3T61546yv1fPtG+C+bx8a97gFMYmjdpLJyzN+e85BAPzzkqM47/6367x+8LXP\nAzDr16dx+u/rDuRHRMZgUn2hZW2fLt8Y3e7QpuY/9esmVLeUXpi5nNPCRNNSjbr7dSD9f2MRyFAi\nMbMyYCNQCex09xIz6wb8EygGyoBz3X1tvGPkmj9cUDMhlOzdNdrfX59rzxxS71hHbU354DlyQHfK\nxo1iZ2UVL88px4EfPFI92D/kuhcaPQYECWXBzSPJyzMqq5xtFZW0b53cf4KvzS2vtzx2vKS2O178\nrEUlkvP+/Bb79+4UHUerz6NvlTGtbC33fPOQ9AUmOSuTXVsnufvB7l4SPi4Fprr7IGBq+FjiePLS\nYXW6vCL27t6uSUlkVxXk5zFiSC9OGdIr7j55BpedtA/zbx7JdWcOqfP8nVM+47w/v8U+1zzLAde/\n0KzFKiurnJufnc3CVZuj3WkQjM3Unsa8ZcdOKiqrmF8edPsc1LfuKstzVmysU5at5q7YyDsL1/Dw\nm2XR1Qdi/e/TlQBcO2Em//lwab37tETzVm7M+pmCuSybxkjOAsaH2+OBszMYS9YzM/LyjI9uOJVT\nhvTi1q8fFH3ulV+clPLzv1l6Mg9ddHiNsoW3jGTBLaP4xWmDyc8zvndsf16+6sQa+9zz0jzeWVjz\neplIayGyLMxT7y1u8NyPvlXG/a8u4KTbX46W1R6bKdm7a7jvIgaNeS5aPvHyY+Met6oquFfM5Cy+\n3uSp96v/Nmu27Kjz/Hcffpe7plR3e34Us3ZaS7Fiw7YaCfC9RWsYccer/CqcGLFjZ1WdRUclszKV\nSByYYmbvmdklYVkvd49MBVoO1Pu118wuMbPpZja9vLz+boxc0qlNIX+5sIRzD+8XnZWVDnt2actJ\ng3tGH//5O4fVO9uruEd7ysaNYs5N8VdB/uof32T2sg3RKcVXPfkhM75YR3HpJE6789U6+9/wn1mN\nxhfpMrvluepp0CfsWwRAl3aF0bI9OrUJ4iydxIBrnuXL977ODx6ZzpvzVjV4/Gv//UmN7r106du1\nXXR70er6B9fvnFJ9R86z//BGymNKpplL13PkzVM5+XcvR8sWrw2u/XliepBE14YJNPK4pdtZWcWm\nFr6mXaYSybHufjBwBnCZmR0f+6QHbdh627Hufr+7l7h7SVFRURpCbTkamrabKmXjRlE2blSjYwyt\nC/LrlMXOPjvjrtdqTCmOfAA2p9tpzk2nc9s5B9Upf/i7QQvqvf87JVq2fMO2eo/xrb++U2/5xm0V\n3D11Lo++vSgjLZd2hdV/x/KN2wHqXdSzX7e2dcpaguikgdXVa8K9FzMWOOaZj9OyiGk6/eSfM/jS\n9S9Eu+6276xscbfgzshgu7svCX+vNLNngCOAFWbW292XmVlvYGUmYpPUe+iiw6Otmdqzz+qzraKS\nNoX5PPTGQrp3aF3n+QU3j2TVpu28OncVZas207ogn56dqj9w+3RpyxulJ0cf5+cZ08YMZ/P2Snp0\naMWBN7zYaAwfL17Pl+99vU55ZHzm12cdwLeP3DvuhZ7NdfW/PubKEYPo2akNVVXOz5+s7s659G/v\nUzZuFC/OXF7ndV+sqb6Cf+O2Cjq2KayzT0tx1IDu0Wnkf3/nc/7+zufR51rajLt5KzcyedZKLj1h\nQPSLX+QL1OxlGxmyZyfunDyX+16Zz5ulJ7Nnl5bxhSDtLRIza29mHSPbwKnAJ8BEYHS422hgQrpj\nk9S66OhiIPhgiDhmYPc4e1cbdstUiksnceN/ZnFFzBX1Fx/bn7ljzyAvz+jZqQ3nHNaXq07bL/p8\n5DqSJTHLokT07NiG/j3aN/gBW1w6iaXrgpuA1ZdEYl03YWaNFZN/899ZHHTDC836Zjn+zTKKSydx\n5+TPeGza5xxx81SAGseP9aO/vx/djk2YESPvrp6avWTdVlbEaYXFemHmcrbuyNzdMgf0aB/d3rEz\n/lI2P3z0vXSEkzQj7niV3z7/KW/NX13nuUffLsPdeXN+0K162wtz0h1es1m6Z0KY2QDgmfBhAfAP\ndx9rZt2BJ4C9gEUE038bXL+8pKTEp09Pfz+1NE9llbNxWwVd2rWqUT70xhdZv7UCgOd+chwdWhfU\nWaW4thnXnVLnOLW5O2f94Q0mXHZMk7r9tlVU4g77X/d8o/vuisi41dwVG7n9xTn84VuH1riGp7Zd\nmcl2y9cOjC68eevXD+Lcw/vVeX3vzm1Ytn4bJw/uyUvhrK6GxtLmrdzIiDteZWi/Lky47Jgmx5IM\np9zxCnOT4tX4AAAOiklEQVRXbgKCyRtmxj/f/ZxfPf1x3Ne0lGtnKqs8ekuI4YN78kA4WaX2+3Xp\nCftw3yvzgdTVzczei5kxm7C0t0jcfYG7Dw1/DnD3sWH5ancf7u6D3H1EY0lEWp78PKv3w3/amOEA\nfP3QvuzfuxP9ugUDyr07t4l7rMaSCARjRhMvP7bJY0dtCvNp2yqfVxuY9fb8T4/j4xtOpWzcKPYp\nCr41d23XcLdRRWUVG7ZVcMqdr/LCzBUMvTF+V9r88k2Nxhn74RJJIgDnHt6v3v2XrQ9aIJEkAtQ7\nlXbKrBUUl05ixB3BBIcPv1jHzsqqtC1uuaB8UzSJANEurEgSqT0DsLkqq5yVGxtvlSVb7EoQhxV3\njW4fNaDmMkh/e7t6NYjIF6xsl03TfyVHtS4I1hj73blDo2UzrjuFKT87oc6+s399esq/ge7VvR23\nf2NojbKJlx/DtGuGM3iPTtHusKk/P5GycaP44LpTOTzmg6G2QWOe46CYcZjNOyqj179E1vyKmFtr\nckG8ZHpk/5ofPrFdhDc2cKFixE2TZke3Zy5dT3HpJL5fzyy0gWOeY9CY57jgr2+zM0kJ5bYXPmXi\nh0sZfO1zFJdOitb55N+9UmO/N2rNnOvavlWdKedAtAtuwowlFJdO4pNGpjxf+rf3OGLsVNZurjt9\nOpUiCR2o0W1Ye/JA7AyulrLsTdq7tpJJXVu7vy/WbKGoYzDAvm5LBXs00EpJtqoqj45LNJa8/jdn\nJd996N1mnads3Cgqq5zFa7dw4YPTWBQzY2nmjaeRn2eYQb5ZuB20sCJdIuO+diDnH7FXjWOu2byD\nzm0LWbR6c50P6Fi/+8bQGgP4jRkzcn/OOnjP6LhNpPupqdy9xvU+9fnRifvwp5fn1ymff/NIKiqr\nokvzNKS+9+uLNVt44PWFPPxmGQCPXnwEU2at4LKTB9KzY+r/XT369iKu/fcnNWLcsmNndGWIYQO6\n89aCmmMn55b05dZzan6pSYYW37Ulsiv6dWtHm8J82hTmpzWJQLDeWGR6c2NO2q/6mprIa+KtPFDb\n0nVbueXZ2Zxw28vRJPLXC0tYeMtI2rcuoE1hPq0L8inIz6v3Q3vkQb3rlHVr34r8PGNAUQemjRke\nbcHUXvCzviRy9zcP4ZLjB9Qb69hnZ0eTCMCNMdf0FJdO4vTf173uJ9a3H6h/WnWsX50+uN7y/Dyj\nTWF+9H45j3zviEaP9dHidVzx2AcUl07iuFv/F00iENzMbfxbizhi7NQa4xTuzvadzZto4O5cP+GT\n6MW1sceNTSIRsevBPXbJUXWej71Wpqqq+XGlmhZtFEmSq88YzD5F1asK1zcV+IqTB3L3S/NqlB09\nru4NyEY0sAxNxNcP7cvT7y+mUyNTe3t2bMM/f1h9X5t4U67vOv9gRh7Ym8L8PL4ydE+OGdiDA/t0\n5tDfTI577IffLOO6M4fwx5eDOn26fCPzyzexdUclndsWRse7It6YV3e2UqwXrzy+3vLYbs7vHzeA\n7x9Xf6KLWLlxGz07tuEr9zb9gszag95nHtSbe78Vf9HT+vz4sQ/470fLGB+z6nW8Vti2isq4i60C\n9O/Rnj7h9N8PPl8bvS3CrrYC00FdWyIpdvOzs7n/1QXRBStP//2rNb6J1nbJ8QO4ZmTdu1QmS1WV\ns2zDNo6JSWD/vuwYDu7XJe5rmrMmGgQrQ7drVf19tfZxFtw8kpsmzebykwfSrX31BIops1ZEx2w+\nuPYUuravf3LFm/NWxb14NJ5vHNaXJxtZhici8qG9YsM2jrx5Kk//aFiD9whqyt9pv14d61xoG5mF\nOPrBabzyWcMrdtx53lC+ekjfJsUfj7q2RFqYa0buT9m4UdEWyu3fGEq7VnWv9IdgvCOVSQSCllKf\nLm359DfBsjW/OG2/BpMIQEFM6+qHJwzg3m81bVXh+Ss34+4ceEPdBTp/dsq+5OUZ1315SI0kAnDI\nXtXxxEsiAEcP7BHtRvzwulN54ofx7yg6Yv9eLLxlJLd9Yyjjw26x//44/tprEFwkuHbzDn75VLDO\n19f/9BZPTP+C4tJJPDF91++uWTZuFFeeMqhOeWQW4vgmdNdd+c/sW2NMLRKRDBn/ZhnXT5zJL0/f\nj1ufDy4+y9ZrIpau28rR417iHz84kqP3Ce6wWXLTFFZt2t6s4zWlnsWlk/jrhSVN6uar/bpYl56w\nD2d8aQ+GxkmWTZkAEE9sPWKvEwEY2rczh+7dlYfeKAPg/MP7MS5cXDU2xv9cfiwH1rMqdX11iTXz\nxtOafRuGZLdIlEhEMqiqypO+rEq6rNm8o8b4ybeP2oubzj6w0e6dF688nn17dWxwn2Q46faXWbhq\nc7RLsSGRmN8dM4L7XpnP0H5daqyiEE9sIvlkyXrOvKd6BYRXfnEie3dvX9/LoucrzDfmjo0/KWN6\n2RrOue+tJp1/VyQ7kWiwXSSDWmoSgWBm2PM/PY7Tf/8aXx66JzedfWCjrzl+36K0JBGA/+3CBYx/\nvOBQJsxYQlHH1lwb3kensUTSqiCPSR8t47W55Tz+bnU316lDenH/hQ1/Rv/3x8dy5j2vM+2aEQ3u\nV1LcrUayaO5YVaqpRSIiSbVo9WbWb62oM2Pq56fsy4+H1x0fyFa1P7T//J3D2LhtJ1c1ct3NnJtO\nr3e162R55K0yjuzfnaKOreuMLTWVWiQiktUi3Tl3nX8wi9du5bKTBkZXcG5JysaN4nsPv8tLn67k\nwmF7c9oBe7B9ZyVXPfkhg/foGHfmXSqTCMCFw4pTevzmUItERKQBU2atqDPgv2j1Zk647eUaZY9f\nclSNla2zmab/ioikUX2zxvbu3p5p1wxn4S0jOSJcNaClJJFUUNeWiEgz9Axv09zQtSu5Qi0SERFJ\niBKJiIgkRIlEREQSokQiIiIJUSIREZGEKJGIiEhClEhERCQhSiQiIpKQFr1EipmVA4sa3TG+HsCq\nJIXTkuRqvUF1V91zS7x67+3uRck6SYtOJIkys+nJXG+mpcjVeoPqrrrnlnTVW11bIiKSECUSERFJ\nSK4nkvszHUCG5Gq9QXXPVbla97TUO6fHSEREJHG53iIREZEEKZGIiEhCcjKRmNnpZjbHzOaZWWmm\n42kuMyszs4/NbIaZTQ/LupnZZDObG/7uGrP/1WGd55jZaTHlh4XHmWdmd5uZheWtzeyfYfk7Zlac\n7jrGxPigma00s09iytJSVzMbHZ5jrpmNTk+Nq8Wp+w1mtiR872eY2ciY53aLuptZPzP7n5nNMrOZ\nZvaTsHy3f98bqHt2vu/unlM/QD4wHxgAtAI+BIZkOq5m1qUM6FGr7FagNNwuBX4bbg8J69oa6B/+\nDfLD56YBRwEGPAecEZb/P+C+cPt84J8ZrOvxwKHAJ+msK9ANWBD+7hpud82Cut8AXFXPvrtN3YHe\nwKHhdkfgs7B+u/373kDds/J9z8UWyRHAPHdf4O47gMeBszIcUzKdBYwPt8cDZ8eUP+7u2919ITAP\nOMLMegOd3P1tD/4VPVLrNZFjPQUMj3ybSTd3fxVYU6s4HXU9DZjs7mvcfS0wGTg9+TWML07d49lt\n6u7uy9z9/XB7IzAb6EMOvO8N1D2ejNY9FxNJH+CLmMeLafgNymYOTDGz98zskrCsl7svC7eXA73C\n7Xj17hNu1y6v8Rp33wmsB7onuxIJSEdds/nfy4/N7KOw6yvSvbNb1j3sdjkEeIcce99r1R2y8H3P\nxUSyOznW3Q8GzgAuM7PjY58Mv4HkxPzuXKpr6E8E3bMHA8uA32U2nNQxsw7A08BP3X1D7HO7+/te\nT92z8n3PxUSyBOgX87hvWNbiuPuS8PdK4BmCbrsVYXOW8PfKcPd49V4Sbtcur/EaMysAOgOrU1GX\nZkpHXbPy34u7r3D3SnevAv5C8N7DblZ3Mysk+CD9u7v/KyzOife9vrpn6/uei4nkXWCQmfU3s1YE\ng0wTMxzTLjOz9mbWMbINnAp8QlCXyCyL0cCEcHsicH44U6M/MAiYFnYRbDCzo8L+0QtrvSZyrHOA\nl8JvgNkiHXV9ATjVzLqG3QinhmUZFfkgDX2V4L2H3ajuYZwPALPd/Y6Yp3b79z1e3bP2fU/HDIRs\n+wFGEsyCmA+MyXQ8zazDAIJZGh8CMyP1IOjjnArMBaYA3WJeMyas8xzCmRtheUn4D3I+cC/VKx60\nAZ4kGLibBgzIYH0fI2jKVxD02V6crroC3wvL5wHfzZK6Pwp8DHwUfiD03t3qDhxL0G31ETAj/BmZ\nC+97A3XPyvddS6SIiEhCcrFrS0REkkiJREREEqJEIiIiCVEiERGRhCiRiIhIQpRIREJmtin8XWxm\n30rysa+p9fjNZB5fJJOUSETqKgZ2KZGEVwY3pEYicfejdzEmkaylRCJS1zjguPB+D1eaWb6Z3WZm\n74aL5f0QwMxONLPXzGwiMCss+3e4iObMyEKaZjYOaBse7+9hWaT1Y+GxP7HgnhHnxRz7ZTN7ysw+\nNbO/Z2rlZZHGNPYtSiQXlRLc8+FMgDAhrHf3w82sNfCGmb0Y7nso8CUPlu4G+J67rzGztsC7Zva0\nu5ea2eUeLLBZ29cIFuAbCvQIX/Nq+NwhwAHAUuAN4Bjg9eRXVyQxapGINO5U4EIzm0GwlHd3grWM\nIFjPaGHMvleY2YfA2wQL3w2iYccCj3mwEN8K4BXg8JhjL/Zggb4ZBF1uIllHLRKRxhnwY3evsXCd\nmZ0IbK71eAQwzN23mNnLBOsZNdf2mO1K9P9VspRaJCJ1bSS4vWnEC8CPwmW9MbN9wxWXa+sMrA2T\nyGCC25tGVEReX8trwHnhOEwRwW11pyWlFiJpom84InV9BFSGXVQPA3cRdCu9Hw54l1N9u9JYzwOX\nmtlsghVY34557n7gIzN7390viCl/BhhGsIqzA7909+VhIhJpEbT6r4iIJERdWyIikhAlEhERSYgS\niYiIJESJREREEqJEIiIiCVEiERGRhCiRiIhIQv4/2Bovh9p19KYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19a487c9d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation with the optimal weights when loss is nan :\n",
      "lSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS"
     ]
    }
   ],
   "source": [
    "# training with RMSprop as optimizer\n",
    "#         ## m, eta, seq_length,K, dSize, n\n",
    "Rnn_RMS = RNN_init(100, 0.001, 25, len(UniChars), len(book_data), 200, 10)\n",
    "weights_Or = InstallWeights(Rnn_RMS)\n",
    "Weights, Loss = MiniBatchGD(book_data, weights_Or, Rnn_RMS, 'RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial loss is: 109.555606379\n",
      "The Smoothed Loss after 0 iterations is :109.546043192\n",
      "Printing generated text:\n",
      "FQz\tL!wOfo^WhNDs(BzE0WrO\tHR 1MDtiWxGVkMl;FRN/WmgL0Gh•}?L}TsZPSK3RZR\n",
      "B /01C zc7X'D2nZ?Bl ERhmDay(0NCqwNaGBkLXT,t sMW\n",
      "BT6\tT\n",
      "'hgqVbQB.u9MiWq'üyg:YUZYr)Bz /tPVU2UE.\n",
      "X^7pU o 6j'S\n",
      "erY1V47n1fP•4L)W\t}\"ohO4efA6\n",
      "\n",
      "The Smoothed Loss after 1000 iterations is :80.9741454707\n",
      "The Smoothed Loss after 2000 iterations is :66.9882117452\n",
      "The Smoothed Loss after 3000 iterations is :60.1811109434\n",
      "The Smoothed Loss after 4000 iterations is :56.585682907\n",
      "The Smoothed Loss after 5000 iterations is :54.5489084338\n",
      "The Smoothed Loss after 6000 iterations is :53.7013125292\n",
      "The Smoothed Loss after 7000 iterations is :53.2675203586\n",
      "The Smoothed Loss after 8000 iterations is :51.1097247211\n",
      "The Smoothed Loss after 9000 iterations is :50.2623503216\n",
      "The Smoothed Loss after 10000 iterations is :50.1681407797\n",
      "Printing generated text:\n",
      "Fare mor'he toon. \"Thamlicg?\"\" sard She werk Mr. Waase watk Her  lait  Mon.\n",
      "\"Turblyearry mlemcorey tamam -tors Harkcaid thast bavars corrwrmanl or mandlugherere acuth rark. . . FDurmidld an at meall sa\n",
      "\n",
      "The Smoothed Loss after 11000 iterations is :50.6438083618\n",
      "The Smoothed Loss after 12000 iterations is :50.3028257111\n",
      "The Smoothed Loss after 13000 iterations is :49.774916435\n",
      "The Smoothed Loss after 14000 iterations is :49.1258263256\n",
      "The Smoothed Loss after 15000 iterations is :48.9543326702\n",
      "The Smoothed Loss after 16000 iterations is :48.3296974715\n",
      "The Smoothed Loss after 17000 iterations is :48.209441288\n",
      "The Smoothed Loss after 18000 iterations is :47.9513180616\n",
      "The Smoothed Loss after 19000 iterations is :47.6960900115\n",
      "The Smoothed Loss after 20000 iterations is :47.1794796337\n",
      "Printing generated text:\n",
      "F- alion's figiSs nfbast.  A'd he gaid beat He him curot her, he'y the in, golly iarit?\" a bitssading had in praiked to bith frsered.\n",
      "\"Loosed, shinly shis's quit, Hazrytikem sow hir been aver him froan\n",
      "\n",
      "The Smoothed Loss after 21000 iterations is :46.9227593395\n",
      "The Smoothed Loss after 22000 iterations is :46.3706909515\n",
      "The Smoothed Loss after 23000 iterations is :46.4192319791\n",
      "The Smoothed Loss after 24000 iterations is :46.354664466\n",
      "The Smoothed Loss after 25000 iterations is :46.4765920277\n",
      "The Smoothed Loss after 26000 iterations is :46.450803693\n",
      "The Smoothed Loss after 27000 iterations is :46.2801028937\n",
      "The Smoothed Loss after 28000 iterations is :46.4555651283\n",
      "The Smoothed Loss after 29000 iterations is :45.9547206212\n",
      "The Smoothed Loss after 30000 iterations is :45.8127035225\n",
      "Printing generated text:\n",
      "Fand- were, bot be ko brilans, himerin og he hor ling tarot acoornetforegweleck they?\" sald he cacel said ale stremor.  Horry sele, his, had Wlabying, mner butsrfece couts you to rer tane of bllolck of\n",
      "\n",
      "The Smoothed Loss after 31000 iterations is :45.9149305449\n",
      "The Smoothed Loss after 32000 iterations is :45.4721451134\n",
      "The Smoothed Loss after 33000 iterations is :45.3499808721\n",
      "The Smoothed Loss after 34000 iterations is :44.4905977733\n",
      "The Smoothed Loss after 35000 iterations is :44.5254836949\n",
      "The Smoothed Loss after 36000 iterations is :44.6561599745\n",
      "The Smoothed Loss after 37000 iterations is :44.8074830644\n",
      "The Smoothed Loss after 38000 iterations is :44.6687687987\n",
      "The Smoothed Loss after 39000 iterations is :44.417245087\n",
      "The Smoothed Loss after 40000 iterations is :44.5208872747\n",
      "Printing generated text:\n",
      "Fich them Vertiectiog gic theme ongolding torted warted dight.  Cold moad mon't -fart agould to fremly, bondodre ,\"I whtre hend  fros'd Harry rolt all,\" there dickness the ckinting to kivet tht nowned-\n",
      "\n",
      "The Smoothed Loss after 41000 iterations is :43.4832720529\n",
      "The Smoothed Loss after 42000 iterations is :43.496124499\n",
      "The Smoothed Loss after 43000 iterations is :43.5782374963\n",
      "The Smoothed Loss after 44000 iterations is :43.6530760772\n",
      "The Smoothed Loss after 45000 iterations is :44.7822669501\n",
      "The Smoothed Loss after 46000 iterations is :45.0437918178\n",
      "The Smoothed Loss after 47000 iterations is :44.9460313312\n",
      "The Smoothed Loss after 48000 iterations is :44.5627987072\n",
      "The Smoothed Loss after 49000 iterations is :44.7510757456\n",
      "The Smoothed Loss after 50000 iterations is :45.3740702597\n",
      "Printing generated text:\n",
      "Fredoollf, of Bads, sard Thead tonch him lookly ity that Mogly remait.\"\n",
      "\"Mumbley seme.\n",
      "\"\"Ol op nourgeadenor, math,\" neyys ling vere, and Je.  \"Ronm.. Mr. WHallestocked atmaich Scouving clable the tiggl\n",
      "\n",
      "The Smoothed Loss after 51000 iterations is :46.4940124666\n",
      "The Smoothed Loss after 52000 iterations is :44.8429476619\n",
      "The Smoothed Loss after 53000 iterations is :43.6226548599\n",
      "The Smoothed Loss after 54000 iterations is :44.2142472172\n",
      "The Smoothed Loss after 55000 iterations is :45.3123159477\n",
      "The Smoothed Loss after 56000 iterations is :45.5677295996\n",
      "The Smoothed Loss after 57000 iterations is :45.1680657554\n",
      "The Smoothed Loss after 58000 iterations is :44.3596336308\n",
      "The Smoothed Loss after 59000 iterations is :45.1044322211\n",
      "The Smoothed Loss after 60000 iterations is :44.16160663\n",
      "Printing generated text:\n",
      "Frain. \"Horw his yous to hastsfore sodered and tonbien off,\"\n",
      "\"Whe tood-joup fnem i've dunHe to repareat into roth,\" shis sin to and s of the Grows!\" sights got insure.\n",
      "\"Fois llay, litF the nome.\"\n",
      "\"E sh\n",
      "\n",
      "The Smoothed Loss after 61000 iterations is :44.1865764274\n",
      "The Smoothed Loss after 62000 iterations is :43.8380920848\n",
      "The Smoothed Loss after 63000 iterations is :44.3294486563\n",
      "The Smoothed Loss after 64000 iterations is :43.8022301296\n",
      "The Smoothed Loss after 65000 iterations is :43.6748443821\n",
      "The Smoothed Loss after 66000 iterations is :43.1369933258\n",
      "The Smoothed Loss after 67000 iterations is :43.5835171723\n",
      "The Smoothed Loss after 68000 iterations is :43.7291228383\n",
      "The Smoothed Loss after 69000 iterations is :43.9509199319\n",
      "The Smoothed Loss after 70000 iterations is :44.0771227403\n",
      "Printing generated text:\n",
      "Fre.  Herm become to knup nesting be nob Pound.  He'd beling way had, Ron likling, his bich teepurow. I sook it spuldent armiug backe the maked of a parys his ind beep,\" said Madiy!\" said Corecone. The\n",
      "\n",
      "The Smoothed Loss after 71000 iterations is :44.3224453227\n",
      "The Smoothed Loss after 72000 iterations is :44.0687794059\n",
      "The Smoothed Loss after 73000 iterations is :43.8793331022\n",
      "The Smoothed Loss after 74000 iterations is :43.7784690261\n",
      "The Smoothed Loss after 75000 iterations is :43.4809353207\n",
      "The Smoothed Loss after 76000 iterations is :43.2212864614\n",
      "The Smoothed Loss after 77000 iterations is :43.0854069752\n",
      "The Smoothed Loss after 78000 iterations is :42.7354736327\n",
      "The Smoothed Loss after 79000 iterations is :41.9549173836\n",
      "The Smoothed Loss after 80000 iterations is :42.4873793929\n",
      "Printing generated text:\n",
      "Fle the goth. . .\"\n",
      "\"he don off helded to sfe reworentore asted out out obles and smool aboin.  Snaighed to had rearis wotte in the enfhing to thing to cading weround him ald'ste had - Harry.  Karls ean\n",
      "\n",
      "The Smoothed Loss after 81000 iterations is :42.2352992564\n",
      "The Smoothed Loss after 82000 iterations is :42.7021345553\n",
      "The Smoothed Loss after 83000 iterations is :42.3976267349\n",
      "The Smoothed Loss after 84000 iterations is :43.2623639199\n",
      "The Smoothed Loss after 85000 iterations is :41.7731867923\n",
      "The Smoothed Loss after 86000 iterations is :42.0570948454\n",
      "The Smoothed Loss after 87000 iterations is :42.1210073615\n",
      "The Smoothed Loss after 88000 iterations is :41.9505203759\n",
      "The Smoothed Loss after 89000 iterations is :42.9234969336\n",
      "The Smoothed Loss after 90000 iterations is :43.093552278\n",
      "Printing generated text:\n",
      "Frabse warted his sur there nourctomy.  He canes Mirger ach ale and and for, and that at Dumble? ,\"Neard Jurble them thing alargely thise astily of was fat?\"  sack nit comm-tomech for fert steptine,e l\n",
      "\n",
      "The Smoothed Loss after 91000 iterations is :43.7322908792\n",
      "The Smoothed Loss after 92000 iterations is :43.2064116537\n",
      "The Smoothed Loss after 93000 iterations is :43.4795339123\n",
      "The Smoothed Loss after 94000 iterations is :44.0458757833\n",
      "The Smoothed Loss after 95000 iterations is :44.9637655872\n",
      "The Smoothed Loss after 96000 iterations is :44.32276657\n",
      "The Smoothed Loss after 97000 iterations is :42.798871002\n",
      "The Smoothed Loss after 98000 iterations is :43.072001788\n",
      "The Smoothed Loss after 99000 iterations is :43.5943696884\n",
      "The Smoothed Loss after 100000 iterations is :44.0396913907\n",
      "Printing generated text:\n",
      "Fre, ssure's the Weasley, werring wioness to meater.\n",
      "\"The fur knom to where'd, Hagrid faighiLugh?\" \"Yeahl. . . Gonedore of whan, Eoksed old tooks out every to my go was tour ragoon, trildeded hoWeswaus\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 101000 iterations is :44.0071983357\n",
      "The Smoothed Loss after 102000 iterations is :43.2711635959\n",
      "The Smoothed Loss after 103000 iterations is :44.0168015807\n",
      "The Smoothed Loss after 104000 iterations is :43.1999465774\n",
      "The Smoothed Loss after 105000 iterations is :42.6700148357\n",
      "The Smoothed Loss after 106000 iterations is :42.3245526581\n",
      "The Smoothed Loss after 107000 iterations is :43.0103634782\n",
      "The Smoothed Loss after 108000 iterations is :42.7362002518\n",
      "The Smoothed Loss after 109000 iterations is :42.572431651\n",
      "The Smoothed Loss after 110000 iterations is :42.2878723119\n",
      "Printing generated text:\n",
      "Fre, leearifn they?\"\n",
      "\n",
      "Tur not did his was eerigh' iver, q to the reels cown be looked hish then a vere diclick ever then  for ficus, himsiou tyearall thom of uplize in it tonk ent of Harry seed to mumm\n",
      "\n",
      "The Smoothed Loss after 111000 iterations is :42.6652948166\n",
      "The Smoothed Loss after 112000 iterations is :42.5361811301\n",
      "The Smoothed Loss after 113000 iterations is :42.8182553412\n",
      "The Smoothed Loss after 114000 iterations is :43.3272757454\n",
      "The Smoothed Loss after 115000 iterations is :43.488989344\n",
      "The Smoothed Loss after 116000 iterations is :42.6251828841\n",
      "The Smoothed Loss after 117000 iterations is :43.5054905742\n",
      "The Smoothed Loss after 118000 iterations is :43.2101278365\n",
      "The Smoothed Loss after 119000 iterations is :42.3873021218\n",
      "The Smoothed Loss after 120000 iterations is :42.7819642529\n",
      "Printing generated text:\n",
      "Furver to coul nhe lewarly squngiof told hat to rum-Werrnaisibly.  \"Herright followh usched to bad staring at the evaios, no? Weaza ters novenaroon Se ror and foot as mull, hadlef the Grow Dudarmed the\n",
      "\n",
      "The Smoothed Loss after 121000 iterations is :42.1263604385\n",
      "The Smoothed Loss after 122000 iterations is :42.471238883\n",
      "The Smoothed Loss after 123000 iterations is :40.8752871089\n",
      "The Smoothed Loss after 124000 iterations is :41.3429009053\n",
      "The Smoothed Loss after 125000 iterations is :41.6110348565\n",
      "The Smoothed Loss after 126000 iterations is :41.8101321781\n",
      "The Smoothed Loss after 127000 iterations is :41.7600932974\n",
      "The Smoothed Loss after 128000 iterations is :42.4574749316\n",
      "The Smoothed Loss after 129000 iterations is :41.5217337554\n",
      "The Smoothed Loss after 130000 iterations is :41.5135550968\n",
      "Printing generated text:\n",
      "Frepull was ffliecline overy parted fawped ou deed its.  Cunsh in, of the morble stull furl mbundre.) Itser ruftes withors.  Sis weat the mulleded a sture lothere a nof go sione see than he was hacend.\n",
      "\n",
      "The Smoothed Loss after 131000 iterations is :40.9471392387\n",
      "The Smoothed Loss after 132000 iterations is :41.0842250079\n",
      "The Smoothed Loss after 133000 iterations is :41.8240783806\n",
      "The Smoothed Loss after 134000 iterations is :42.5862934019\n",
      "The Smoothed Loss after 135000 iterations is :42.9592311\n",
      "The Smoothed Loss after 136000 iterations is :42.6606310592\n",
      "The Smoothed Loss after 137000 iterations is :42.7604745456\n",
      "The Smoothed Loss after 138000 iterations is :43.0567220963\n",
      "The Smoothed Loss after 139000 iterations is :43.5443896795\n",
      "The Smoothed Loss after 140000 iterations is :44.6852788939\n",
      "Printing generated text:\n",
      "Fren's and house in begave.  \"No sastly Charrysurgeen and callide.  Har tis they seening and Harry bigertamanther.\n",
      "\"Thange arman a goter osly on't fach.  It was coppand to heleed thom a was Qut's get s\n",
      "\n",
      "The Smoothed Loss after 141000 iterations is :42.6235928907\n",
      "The Smoothed Loss after 142000 iterations is :42.1722299546\n",
      "The Smoothed Loss after 143000 iterations is :42.7269860894\n",
      "The Smoothed Loss after 144000 iterations is :43.644399394\n",
      "The Smoothed Loss after 145000 iterations is :43.7859067056\n",
      "The Smoothed Loss after 146000 iterations is :42.9608797925\n",
      "The Smoothed Loss after 147000 iterations is :43.4442954192\n",
      "The Smoothed Loss after 148000 iterations is :42.8376135358\n",
      "The Smoothed Loss after 149000 iterations is :42.2689626763\n",
      "The Smoothed Loss after 150000 iterations is :42.0137218753\n",
      "Printing generated text:\n",
      "Fred.\n",
      "\"Did had Morbblooshisile, gonted apaid...\n",
      "IAm Beong care .\"\n",
      "\"I've goinnano legahed.\"\n",
      " Creet into he I wapted Frearys to mach!\" Harry conced, acpeled to ely. . .\"\n",
      "\"Ceary tow cho!\"\n",
      "\tWat out when he\n",
      "\n",
      "The Smoothed Loss after 151000 iterations is :42.1625481013\n",
      "The Smoothed Loss after 152000 iterations is :42.3729190843\n",
      "The Smoothed Loss after 153000 iterations is :41.8330133625\n",
      "The Smoothed Loss after 154000 iterations is :41.8237844102\n",
      "The Smoothed Loss after 155000 iterations is :41.6579878823\n",
      "The Smoothed Loss after 156000 iterations is :41.6730281734\n",
      "The Smoothed Loss after 157000 iterations is :41.9663545186\n",
      "The Smoothed Loss after 158000 iterations is :42.6196660071\n",
      "The Smoothed Loss after 159000 iterations is :42.5577813186\n",
      "The Smoothed Loss after 160000 iterations is :42.3975944947\n",
      "Printing generated text:\n",
      "Flen, agr comently?\"\n",
      "Bit'm walkiund, and coubchor clouge doje?\"\n",
      "\"I a make, whrish thenk neth leked than wikbow with by he sigmus uth and doing gone palk wapped, Who im alo saw the Ixprepss a who said; \n",
      "\n",
      "The Smoothed Loss after 161000 iterations is :42.9532975374\n",
      "The Smoothed Loss after 162000 iterations is :42.3057852921\n",
      "The Smoothed Loss after 163000 iterations is :42.456641834\n",
      "The Smoothed Loss after 164000 iterations is :42.6499297218\n",
      "The Smoothed Loss after 165000 iterations is :41.9020248188\n",
      "The Smoothed Loss after 166000 iterations is :42.1438341301\n",
      "The Smoothed Loss after 167000 iterations is :40.8137070391\n",
      "The Smoothed Loss after 168000 iterations is :41.2438671343\n",
      "The Smoothed Loss after 169000 iterations is :41.3770417325\n",
      "The Smoothed Loss after 170000 iterations is :41.7264891731\n",
      "Printing generated text:\n",
      "Fugs. Thered ought'w rehaid Bagmanion Krum calle-stern yhavied. . . than it again The kning it tom innore the the just expect.\" Snap the waing.  Harry ap that set onttaid off thas not the folss an them\n",
      "\n",
      "The Smoothed Loss after 171000 iterations is :41.7194798347\n",
      "The Smoothed Loss after 172000 iterations is :41.5901802861\n",
      "The Smoothed Loss after 173000 iterations is :41.5000203727\n",
      "The Smoothed Loss after 174000 iterations is :40.765683379\n",
      "The Smoothed Loss after 175000 iterations is :40.5693406093\n",
      "The Smoothed Loss after 176000 iterations is :40.8163335278\n",
      "The Smoothed Loss after 177000 iterations is :40.8003766913\n",
      "The Smoothed Loss after 178000 iterations is :42.3827080617\n",
      "The Smoothed Loss after 179000 iterations is :42.8088513159\n",
      "The Smoothed Loss after 180000 iterations is :42.5352129163\n",
      "Printing generated text:\n",
      "Folly befar, If the mante!\" The Waych Harry where he has had timeye..\"\n",
      "Unf cay somstesouncted cerser, Wexider).  \"I neked my Muss -\"\n",
      "\"Near  thing Volly netry wabed dri've goidelys finge incwald of Harr\n",
      "\n",
      "The Smoothed Loss after 181000 iterations is :42.4914959251\n",
      "The Smoothed Loss after 182000 iterations is :42.5786502583\n",
      "The Smoothed Loss after 183000 iterations is :43.3946689265\n",
      "The Smoothed Loss after 184000 iterations is :44.7542748871\n",
      "The Smoothed Loss after 185000 iterations is :42.6570514071\n",
      "The Smoothed Loss after 186000 iterations is :41.7364501125\n",
      "The Smoothed Loss after 187000 iterations is :42.2259180367\n",
      "The Smoothed Loss after 188000 iterations is :43.3582313008\n",
      "The Smoothed Loss after 189000 iterations is :43.6173889306\n",
      "The Smoothed Loss after 190000 iterations is :42.9941413405\n",
      "Printing generated text:\n",
      "Fred broocher ficling lough were Lull, are wfile bur for it.  Ancion barry's swill Miggle, one fin were at.  Harry backed to \"It ibis, eave terridled the Rendid,\" said Mable a sting into the was brause\n",
      "\n",
      "The Smoothed Loss after 191000 iterations is :42.3096151699\n",
      "The Smoothed Loss after 192000 iterations is :42.8899611618\n",
      "The Smoothed Loss after 193000 iterations is :42.0490661339\n",
      "The Smoothed Loss after 194000 iterations is :41.9126491234\n",
      "The Smoothed Loss after 195000 iterations is :41.6828839218\n",
      "The Smoothed Loss after 196000 iterations is :42.1633602786\n",
      "The Smoothed Loss after 197000 iterations is :41.850121728\n",
      "The Smoothed Loss after 198000 iterations is :41.6327906389\n",
      "The Smoothed Loss after 199000 iterations is :41.145089319\n",
      "The Smoothed Loss after 200000 iterations is :41.6685734764\n",
      "Printing generated text:\n",
      "Frepurn sights off open around with him an Harry, sand to get beft a of that'r marmious froce.  Where bothtrraads. . . bet, benost,\" \"Dor, that sise, so abowed Hondee - a knows - I'm me, he head turnai\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 201000 iterations is :42.0312649609\n",
      "The Smoothed Loss after 202000 iterations is :42.2986092811\n",
      "The Smoothed Loss after 203000 iterations is :42.4353823137\n",
      "The Smoothed Loss after 204000 iterations is :42.3230288926\n",
      "The Smoothed Loss after 205000 iterations is :42.5277292594\n",
      "The Smoothed Loss after 206000 iterations is :42.0345341136\n",
      "The Smoothed Loss after 207000 iterations is :42.055566064\n",
      "The Smoothed Loss after 208000 iterations is :42.109788936\n",
      "The Smoothed Loss after 209000 iterations is :41.6214508993\n",
      "The Smoothed Loss after 210000 iterations is :41.7772316815\n",
      "Printing generated text:\n",
      "F Hagrest watre.\n",
      "\"I hir belions be the noved like erenk Bagmueleled onned, and Herst as Harry and Greny he up!\"  he uping hongs and gonge.  WerrusuPe gavais!\"\n",
      "\n",
      "The hand, Pagpyo loiling Croucing and bes\n",
      "\n",
      "The Smoothed Loss after 211000 iterations is :40.8585346923\n",
      "The Smoothed Loss after 212000 iterations is :40.5785597103\n",
      "The Smoothed Loss after 213000 iterations is :41.064196229\n",
      "The Smoothed Loss after 214000 iterations is :40.6937234235\n",
      "The Smoothed Loss after 215000 iterations is :41.149700118\n",
      "The Smoothed Loss after 216000 iterations is :40.9246472102\n",
      "The Smoothed Loss after 217000 iterations is :41.6685327407\n",
      "The Smoothed Loss after 218000 iterations is :40.3934799265\n",
      "The Smoothed Loss after 219000 iterations is :40.6035636416\n",
      "The Smoothed Loss after 220000 iterations is :40.6588950646\n",
      "Printing generated text:\n",
      "Fretterid.  You kets t want,\" see to my cair mise.  \"She ace.  Harry was lusting in and for for coud Moobysed looking the deelice, Snate was bunstoreshed in hir, ewors.  Wismime.  Ron, but she Deached \n",
      "\n",
      "The Smoothed Loss after 221000 iterations is :40.5478352364\n",
      "The Smoothed Loss after 222000 iterations is :41.7124687498\n",
      "The Smoothed Loss after 223000 iterations is :41.711153147\n",
      "The Smoothed Loss after 224000 iterations is :42.4884325227\n",
      "The Smoothed Loss after 225000 iterations is :41.865126952\n",
      "The Smoothed Loss after 226000 iterations is :42.3662370041\n",
      "The Smoothed Loss after 227000 iterations is :42.8768513337\n",
      "The Smoothed Loss after 228000 iterations is :44.1309532272\n",
      "The Smoothed Loss after 229000 iterations is :43.032256533\n",
      "The Smoothed Loss after 230000 iterations is :41.3496448159\n",
      "Printing generated text:\n",
      "Flangerting ghatted toone . . . \"Whts it stricting there's polled theser snargh, the ganing not's fintle.\n",
      "\"I he sort. Llosled on, her !\"Thened. \"Which loudd he's wafono, saudd itus. Weally a lating som\n",
      "\n",
      "The Smoothed Loss after 231000 iterations is :41.7861298449\n",
      "The Smoothed Loss after 232000 iterations is :42.3645521734\n",
      "The Smoothed Loss after 233000 iterations is :42.8547901076\n",
      "The Smoothed Loss after 234000 iterations is :42.761619099\n",
      "The Smoothed Loss after 235000 iterations is :42.2507294392\n",
      "The Smoothed Loss after 236000 iterations is :42.8052686524\n",
      "The Smoothed Loss after 237000 iterations is :41.6771749654\n",
      "The Smoothed Loss after 238000 iterations is :41.5618763676\n",
      "The Smoothed Loss after 239000 iterations is :40.996757176\n",
      "The Smoothed Loss after 240000 iterations is :41.8076837167\n",
      "Printing generated text:\n",
      "Find had befire's all mantion.\n",
      "\"See was he quouth abone the lotarvers.  O!\"\n",
      "said Harry fake, \" quink, of was to somew not sever of task thickear.  Harry spel, mear fell heht licl - a met.\n",
      "Stephing nore\n",
      "\n",
      "The Smoothed Loss after 241000 iterations is :41.380062037\n",
      "The Smoothed Loss after 242000 iterations is :41.2320198476\n",
      "The Smoothed Loss after 243000 iterations is :41.2207906573\n",
      "The Smoothed Loss after 244000 iterations is :41.7984197678\n",
      "The Smoothed Loss after 245000 iterations is :41.4811788709\n",
      "The Smoothed Loss after 246000 iterations is :41.730873812\n",
      "The Smoothed Loss after 247000 iterations is :42.0944817538\n",
      "The Smoothed Loss after 248000 iterations is :42.7991967369\n",
      "The Smoothed Loss after 249000 iterations is :41.9034182993\n",
      "The Smoothed Loss after 250000 iterations is :42.4959473647\n",
      "Printing generated text:\n",
      "Flitt-\n",
      "\"Sobetainsaing is the toplow to very Snape . Moody swe-tion.\n",
      "His into the nhiose rech aftet and I't -ong to maggh now Hermione shred we poothents yesa istol's the do to carssougo innons himp to \n",
      "\n",
      "The Smoothed Loss after 251000 iterations is :42.1499297255\n",
      "The Smoothed Loss after 252000 iterations is :41.4790084464\n",
      "The Smoothed Loss after 253000 iterations is :41.8125971661\n",
      "The Smoothed Loss after 254000 iterations is :41.2371150361\n",
      "The Smoothed Loss after 255000 iterations is :41.3811884473\n",
      "The Smoothed Loss after 256000 iterations is :39.8822680211\n",
      "The Smoothed Loss after 257000 iterations is :40.4357071636\n",
      "The Smoothed Loss after 258000 iterations is :40.5150135336\n",
      "The Smoothed Loss after 259000 iterations is :40.8426146483\n",
      "The Smoothed Loss after 260000 iterations is :40.7660782335\n",
      "Printing generated text:\n",
      "F Harry. I Trinker, he ropt, as got on this bor' fell of the might on thenr had be to thinksolf I was looked this lothin walk on the his stooks gortle ittran; When tell to said wand, and siticulief lep\n",
      "\n",
      "The Smoothed Loss after 261000 iterations is :41.5389167851\n",
      "The Smoothed Loss after 262000 iterations is :40.5119684482\n",
      "The Smoothed Loss after 263000 iterations is :40.4542592143\n",
      "The Smoothed Loss after 264000 iterations is :40.1337235889\n",
      "The Smoothed Loss after 265000 iterations is :40.0058294741\n",
      "The Smoothed Loss after 266000 iterations is :41.1649992426\n",
      "The Smoothed Loss after 267000 iterations is :41.7762124979\n",
      "The Smoothed Loss after 268000 iterations is :42.0501699838\n",
      "The Smoothed Loss after 269000 iterations is :41.6591138752\n",
      "The Smoothed Loss after 270000 iterations is :41.9910925527\n",
      "Printing generated text:\n",
      "F Mr. Weasley's ave too joine a his hoady foot expepplear it seft went; you wang let to McGoblious her at the ronggred.\"\n",
      "He was lere!\"\n",
      "\"Grally bothing very and in to reartly to as out was feacesenss as\n",
      "\n",
      "The Smoothed Loss after 271000 iterations is :42.3755224814\n",
      "The Smoothed Loss after 272000 iterations is :42.9351010141\n",
      "The Smoothed Loss after 273000 iterations is :43.6972952918\n",
      "The Smoothed Loss after 274000 iterations is :41.700678239\n",
      "The Smoothed Loss after 275000 iterations is :41.6302972843\n",
      "The Smoothed Loss after 276000 iterations is :41.9917977571\n",
      "The Smoothed Loss after 277000 iterations is :42.850534036\n",
      "The Smoothed Loss after 278000 iterations is :43.1533631811\n",
      "The Smoothed Loss after 279000 iterations is :41.9610186432\n",
      "The Smoothed Loss after 280000 iterations is :42.6100282037\n",
      "Printing generated text:\n",
      "Fred Ron slaving she mumbled Lurgand thear momentrowe uvetud you:  Harry frest all everywhet Filled canced the hages they to Hermioned up to experalformed, I come mutting to the riding of off. Hagrid h\n",
      "\n",
      "The Smoothed Loss after 281000 iterations is :41.9178319574\n",
      "The Smoothed Loss after 282000 iterations is :41.4841337823\n",
      "The Smoothed Loss after 283000 iterations is :41.0470859028\n",
      "The Smoothed Loss after 284000 iterations is :41.4684120324\n",
      "The Smoothed Loss after 285000 iterations is :41.5582371559\n",
      "The Smoothed Loss after 286000 iterations is :40.8655258985\n",
      "The Smoothed Loss after 287000 iterations is :40.9145289467\n",
      "The Smoothed Loss after 288000 iterations is :41.0513695958\n",
      "The Smoothed Loss after 289000 iterations is :40.9742191087\n",
      "The Smoothed Loss after 290000 iterations is :41.1914072574\n",
      "Printing generated text:\n",
      "Fren persers.\n",
      "\"They Glange her dask Blowle ching of Ron paring we looking in theur?\"  have? \"Yeach's legers!\"\n",
      "\"Yourth raster, I'd gol of my quiartured justly with glat,  \"I'm, very, jaicaid.  \"A'd Croa\n",
      "\n",
      "The Smoothed Loss after 291000 iterations is :42.1813672966\n",
      "The Smoothed Loss after 292000 iterations is :41.6755338663\n",
      "The Smoothed Loss after 293000 iterations is :41.6872209679\n",
      "The Smoothed Loss after 294000 iterations is :42.3013193202\n",
      "The Smoothed Loss after 295000 iterations is :41.7322595611\n",
      "The Smoothed Loss after 296000 iterations is :41.7152952143\n",
      "The Smoothed Loss after 297000 iterations is :41.999608225\n",
      "The Smoothed Loss after 298000 iterations is :41.0576152367\n",
      "The Smoothed Loss after 299000 iterations is :41.5261292763\n",
      "The Smoothed Loss after 300000 iterations is :40.0032185817\n",
      "Printing generated text:\n",
      "F toirt looked Harry queled.\"\n",
      "Patter afting owl stallo.  You take at him. Harry, his surimed to now has was neatle louddre sharie!  Harry?\" see up to they platting with awight he wizpured-arore know, s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 301000 iterations is :40.5704854658\n",
      "The Smoothed Loss after 302000 iterations is :40.808699696\n",
      "The Smoothed Loss after 303000 iterations is :40.8593615268\n",
      "The Smoothed Loss after 304000 iterations is :40.9974437207\n",
      "The Smoothed Loss after 305000 iterations is :41.0420630453\n",
      "The Smoothed Loss after 306000 iterations is :40.6861532773\n",
      "The Smoothed Loss after 307000 iterations is :40.1162200234\n",
      "The Smoothed Loss after 308000 iterations is :39.7898273623\n",
      "The Smoothed Loss after 309000 iterations is :39.9493199146\n",
      "The Smoothed Loss after 310000 iterations is :40.1662661415\n",
      "Printing generated text:\n",
      "F Geck drins, what po ingill will Cedly bagr, difforce.  Roo get's wevo dad's ou he down and alant a a both the lack caund.  But 'll judgiels bly, a read I'l jurvodowhe frot the bearge mesy way uncug, \n",
      "\n",
      "The Smoothed Loss after 311000 iterations is :41.6865575763\n",
      "The Smoothed Loss after 312000 iterations is :42.2712134458\n",
      "The Smoothed Loss after 313000 iterations is :41.638987613\n",
      "The Smoothed Loss after 314000 iterations is :41.7661719114\n",
      "The Smoothed Loss after 315000 iterations is :42.1818195206\n",
      "The Smoothed Loss after 316000 iterations is :42.6733068352\n",
      "The Smoothed Loss after 317000 iterations is :44.1568050269\n",
      "The Smoothed Loss after 318000 iterations is :41.8057142549\n",
      "The Smoothed Loss after 319000 iterations is :41.2600317429\n",
      "The Smoothed Loss after 320000 iterations is :41.421149196\n",
      "Printing generated text:\n",
      "FFlach, who sore with ally.\n",
      "Harry, and parn his Irouset. . aloudd, unto ahame Ritor, I've been have Eanit.\n",
      "\"I've stared and ongitane,\" Harry snidges; deet.\n",
      "\"Harry ard nized; exarley neving struced nom.\n",
      "\n",
      "The Smoothed Loss after 321000 iterations is :42.8421152067\n",
      "The Smoothed Loss after 322000 iterations is :42.8501672853\n",
      "The Smoothed Loss after 323000 iterations is :42.3307323965\n",
      "The Smoothed Loss after 324000 iterations is :41.746249652\n",
      "The Smoothed Loss after 325000 iterations is :42.1150110068\n",
      "The Smoothed Loss after 326000 iterations is :41.4528119647\n",
      "The Smoothed Loss after 327000 iterations is :41.10841796\n",
      "The Smoothed Loss after 328000 iterations is :41.1678544923\n",
      "The Smoothed Loss after 329000 iterations is :41.4746795966\n",
      "The Smoothed Loss after 330000 iterations is :41.0545561736\n",
      "Printing generated text:\n",
      "Fire not and ot in a'm.\n",
      "\"A'd beyth the dark?\"\n",
      "\"Fulged a litt turn chowidur, slecs ashee Maxir, Harry, said Harry fiing blachad to kild to Some not and about herwdn't snowing Sore; Right studters.  Antr\n",
      "\n",
      "The Smoothed Loss after 331000 iterations is :40.9686502189\n",
      "The Smoothed Loss after 332000 iterations is :40.4651237372\n",
      "The Smoothed Loss after 333000 iterations is :41.0162627071\n",
      "The Smoothed Loss after 334000 iterations is :41.4910112078\n",
      "The Smoothed Loss after 335000 iterations is :41.6925483096\n",
      "The Smoothed Loss after 336000 iterations is :41.824000725\n",
      "The Smoothed Loss after 337000 iterations is :41.6109261477\n",
      "The Smoothed Loss after 338000 iterations is :42.140642281\n",
      "The Smoothed Loss after 339000 iterations is :41.4476498935\n",
      "The Smoothed Loss after 340000 iterations is :41.6394660872\n",
      "Printing generated text:\n",
      "Flene. . . . and Berparousee surethirg lealled. . . Geet to spulled ome.\n",
      "\"Ml. . . as gresed him mo the endiled gikled.  HOgh to he'r judn't wans set of he dhe sald, and the wey' twind greas.  bust Meld\n",
      "\n",
      "The Smoothed Loss after 341000 iterations is :41.9403426541\n",
      "The Smoothed Loss after 342000 iterations is :41.0281213598\n",
      "The Smoothed Loss after 343000 iterations is :41.419052322\n",
      "The Smoothed Loss after 344000 iterations is :40.1818953553\n",
      "The Smoothed Loss after 345000 iterations is :40.0137160599\n",
      "The Smoothed Loss after 346000 iterations is :40.5216191873\n",
      "The Smoothed Loss after 347000 iterations is :40.3942288513\n",
      "The Smoothed Loss after 348000 iterations is :40.5127062417\n",
      "The Smoothed Loss after 349000 iterations is :40.5355241892\n",
      "The Smoothed Loss after 350000 iterations is :41.0011545128\n",
      "Printing generated text:\n",
      "Facking one, was aboup. . . . . . yeve, briaty, all roge.  \"Voke, wathing sers.\n",
      "Moody corsy flien enta wrow, was Loded a,\" saily was offert..\"\n",
      "Pat it her now a you retaied it was hil to we looking no, \n",
      "\n",
      "The Smoothed Loss after 351000 iterations is :40.0565331208\n",
      "The Smoothed Loss after 352000 iterations is :40.0435015858\n",
      "The Smoothed Loss after 353000 iterations is :40.1124289576\n",
      "The Smoothed Loss after 354000 iterations is :39.9357467521\n",
      "The Smoothed Loss after 355000 iterations is :41.4430199335\n",
      "The Smoothed Loss after 356000 iterations is :41.4991945499\n",
      "The Smoothed Loss after 357000 iterations is :42.1110428455\n",
      "The Smoothed Loss after 358000 iterations is :41.347533512\n",
      "The Smoothed Loss after 359000 iterations is :41.7894392752\n",
      "The Smoothed Loss after 360000 iterations is :42.5052965836\n",
      "Printing generated text:\n",
      "Forn!\" said Ron belled you.  Bightragounrate, like gley read.\n",
      "Harryt' this?\"\n",
      "\"Durchounts?\"\n",
      "\"Harry along who dasting was they passive. \"Soire was want vereaned than mave tandly atrle's patengenting it t\n",
      "\n",
      "The Smoothed Loss after 361000 iterations is :43.8221369656\n",
      "The Smoothed Loss after 362000 iterations is :42.4602600675\n",
      "The Smoothed Loss after 363000 iterations is :40.7652548493\n",
      "The Smoothed Loss after 364000 iterations is :41.3946622416\n",
      "The Smoothed Loss after 365000 iterations is :42.0480075629\n",
      "The Smoothed Loss after 366000 iterations is :42.3934285185\n",
      "The Smoothed Loss after 367000 iterations is :42.3573937561\n",
      "The Smoothed Loss after 368000 iterations is :41.7113527934\n",
      "The Smoothed Loss after 369000 iterations is :42.3587665387\n",
      "The Smoothed Loss after 370000 iterations is :41.1102154155\n",
      "Printing generated text:\n",
      "Fland furousht the mevered up than he wyong Warcy the jump at through the ceptong, and thriighing to tre judging now at they wended woch ecore.\n",
      "\"I'd ball a madeslation.  I'm Cedreg fied looked arthindo\n",
      "\n",
      "The Smoothed Loss after 371000 iterations is :40.9466563744\n",
      "The Smoothed Loss after 372000 iterations is :40.6433202842\n",
      "The Smoothed Loss after 373000 iterations is :41.5121429097\n",
      "The Smoothed Loss after 374000 iterations is :40.9073089098\n",
      "The Smoothed Loss after 375000 iterations is :40.7521484635\n",
      "The Smoothed Loss after 376000 iterations is :40.6704443909\n",
      "The Smoothed Loss after 377000 iterations is :41.1266889128\n",
      "The Smoothed Loss after 378000 iterations is :41.2150998056\n",
      "The Smoothed Loss after 379000 iterations is :41.3223810984\n",
      "The Smoothed Loss after 380000 iterations is :41.6526507918\n",
      "Printing generated text:\n",
      "Fileaked Piopyening were nextesed the hend were looking not be you..\n",
      "Bakbem once.\n",
      "\"You're if the big thaikily as nett an owhersy ask whis pearing clope tuager just-Harry jujt Karking the who deen one h\n",
      "\n",
      "The Smoothed Loss after 381000 iterations is :42.1529798228\n",
      "The Smoothed Loss after 382000 iterations is :41.6514131502\n",
      "The Smoothed Loss after 383000 iterations is :41.8892508725\n",
      "The Smoothed Loss after 384000 iterations is :41.7117467905\n",
      "The Smoothed Loss after 385000 iterations is :41.3505119663\n",
      "The Smoothed Loss after 386000 iterations is :41.3323774211\n",
      "The Smoothed Loss after 387000 iterations is :40.7371122101\n",
      "The Smoothed Loss after 388000 iterations is :40.7921725307\n",
      "The Smoothed Loss after 389000 iterations is :39.4323636399\n",
      "The Smoothed Loss after 390000 iterations is :40.2270737414\n",
      "Printing generated text:\n",
      "F a finging icross Medy Crooly seemen't be there tor-ter, as did, takened.  \"OuU He wasn't froming...\"\n",
      "The mice frodg.\"\n",
      "Birpied an Moody my pecousced,\" Harry Harry, bennotered insic. Not somiose undeut\n",
      "\n",
      "The Smoothed Loss after 391000 iterations is :39.908038992\n",
      "The Smoothed Loss after 392000 iterations is :40.3130667258\n",
      "The Smoothed Loss after 393000 iterations is :40.1866293235\n",
      "The Smoothed Loss after 394000 iterations is :41.447376116\n",
      "The Smoothed Loss after 395000 iterations is :40.010918834\n",
      "The Smoothed Loss after 396000 iterations is :40.2014539445\n",
      "The Smoothed Loss after 397000 iterations is :39.7235002127\n",
      "The Smoothed Loss after 398000 iterations is :39.7189158644\n",
      "The Smoothed Loss after 399000 iterations is :41.029489305\n",
      "The Smoothed Loss after 400000 iterations is :41.2568729894\n",
      "Printing generated text:\n",
      "Fredor, \"ze unkered who ye when Harry while before heard to clasir?  Whands of his bestay that he coint everyth facion.\"\n",
      "Flact, head gosk...\"\n",
      "Hoff heardick toum flulg carra. .\n",
      "\n",
      "He was it, and ond off s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Smoothed Loss after 401000 iterations is :41.905752078\n",
      "The Smoothed Loss after 402000 iterations is :41.2228819423\n",
      "The Smoothed Loss after 403000 iterations is :41.6469579685\n",
      "The Smoothed Loss after 404000 iterations is :42.15820107\n",
      "The Smoothed Loss after 405000 iterations is :42.6201504166\n",
      "The Smoothed Loss after 406000 iterations is :43.0171256071\n",
      "The Smoothed Loss after 407000 iterations is :41.2984068364\n",
      "The Smoothed Loss after 408000 iterations is :41.2498221672\n",
      "The Smoothed Loss after 409000 iterations is :41.7333925613\n",
      "The Smoothed Loss after 410000 iterations is :42.4383447129\n",
      "Printing generated text:\n",
      "F remay usun baikher the Goors stan fast sig and people be Diddory at his reshoulderacting him mome belet uping to swillidesn as sorried leaman at startion.\n",
      "\"You, the stoot moves, engrocre spachinu hea\n",
      "\n",
      "The Smoothed Loss after 411000 iterations is :42.5540336236\n",
      "The Smoothed Loss after 412000 iterations is :41.7235450473\n",
      "The Smoothed Loss after 413000 iterations is :42.3641154042\n",
      "The Smoothed Loss after 414000 iterations is :41.5066289485\n",
      "The Smoothed Loss after 415000 iterations is :40.8419937528\n",
      "The Smoothed Loss after 416000 iterations is :40.5783776064\n",
      "The Smoothed Loss after 417000 iterations is :41.2218372529\n",
      "The Smoothed Loss after 418000 iterations is :41.1586314983\n",
      "The Smoothed Loss after 419000 iterations is :40.5137157549\n",
      "The Smoothed Loss after 420000 iterations is :40.6158315553\n",
      "Printing generated text:\n",
      "Fared the suping, looked in been pernarside hoonautok as they clacking, ent to read, aflech of Harry knew a mustered the good-  his maded; the and that it only famed to wasch in twes mursed around uria\n",
      "\n",
      "The Smoothed Loss after 421000 iterations is :40.8474335405\n",
      "The Smoothed Loss after 422000 iterations is :40.724555781\n",
      "The Smoothed Loss after 423000 iterations is :40.9566339989\n",
      "The Smoothed Loss after 424000 iterations is :41.8653323367\n",
      "The Smoothed Loss after 425000 iterations is :41.3397447079\n",
      "The Smoothed Loss after 426000 iterations is :41.0989827899\n",
      "The Smoothed Loss after 427000 iterations is :42.13435737\n",
      "The Smoothed Loss after 428000 iterations is :41.466772435\n",
      "The Smoothed Loss after 429000 iterations is :41.240741089\n",
      "The Smoothed Loss after 430000 iterations is :41.5040347245\n",
      "Printing generated text:\n",
      "Fred in't be and persent.  She waine like.\n",
      "\"Harry off overs for Gryme'a ... Seraches caving where of his denter feet she said I'm of noren has bogor sell., Snacthing a fintiling where restay, who quely\n",
      "\n",
      "The Smoothed Loss after 431000 iterations is :40.6050588523\n",
      "The Smoothed Loss after 432000 iterations is :41.1338485527\n",
      "The Smoothed Loss after 433000 iterations is :39.4591024486\n",
      "The Smoothed Loss after 434000 iterations is :40.1621814259\n",
      "The Smoothed Loss after 435000 iterations is :40.4056195967\n",
      "The Smoothed Loss after 436000 iterations is :40.2610562102\n",
      "The Smoothed Loss after 437000 iterations is :40.467720043\n",
      "The Smoothed Loss after 438000 iterations is :40.9255451899\n",
      "The Smoothed Loss after 439000 iterations is :40.213615005\n",
      "The Smoothed Loss after 440000 iterations is :39.9692282736\n",
      "Printing generated text:\n",
      "Fred Wondore out aflever,\" saudn't bast and Mat ho, should alrod, he?  \n",
      "Wellatcling previents ont out anyiss. . . Yous sustle, he'd for the me to panded to in the Draumiapieps the bout the bus, \"hire a\n",
      "\n",
      "The Smoothed Loss after 441000 iterations is :39.5134600303\n",
      "The Smoothed Loss after 442000 iterations is :39.4130207875\n",
      "The Smoothed Loss after 443000 iterations is :39.8741366426\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ad084a77f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation with the optimal weights when loss is 39.3386255379 :\n",
      "Flarg thim wring dietrising no was someage, for it their he high, tall, and Sirion her he was gornar.\"\n",
      "Hark and will Dumblawd freaged, vele to thened snitcend, and shrulks con snipped ahauxing whele, lather next that his couldet, he chenk and surdfort on the been,\" said Ron.\n",
      "\"Trowiapfight. youring on Wensamat to sloy going and wes -\"\n",
      "\"But he he her the start. . a into the parky)?\"  should were; this.  They feet to,\" Hadre lack rointier fat tear to cargure'l, and sise for the mostinntureage sethered to then whreden co in the Ritawsing gire you?\"\n",
      "Ron and well head on Velt, \"rewtly word-Wirhirsmantion,\" said Firsion the Trulked yearted in maters.  Hermione for frot-Lory heavor ligme\"\n",
      "\"You'd in kith,\" said George back, work expetumin he coplo kedle.\n",
      "\"Tthat neght came dowhable he hoteely.  I whing in the kaving staged in the sire wizandy was pare, the . bet beHire mullinned at the troumlame mark over.\n",
      "\" Trinked thin cloak mirht thought.  Fredelid you!\"\n",
      "Herwinned your Harry, murbinions good, "
     ]
    }
   ],
   "source": [
    "# training with AdaGrad as optimizer\n",
    "#         ## m, eta, seq_length,K, dSize, n\n",
    "RNN_Ada = RNN_init(100, 0.1, 25, len(UniChars), len(book_data), 200, 10)\n",
    "weights_Or = InstallWeights(RNN_Ada)\n",
    "Weights, Loss = MiniBatchGD(book_data, weights_Or, RNN_Ada, 'AdaGrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
